[{"content":"GitHub Template roblox-ts-rojo-template\nEnglish Êó•Êú¨Ë™û\nWhat is this? This is a template for a modern development environment using VSCode for Roblox (TypeScript).\nVSCode üëâ Roblox Studio sync: Rojo Linter: ESlint Formatter: Prettier Package Manager: pnpm ‚ùó Caution Functionality is unconfirmed, but popular packages have been included.\n\u0026ldquo;@rbxts/janitor\u0026rdquo; \u0026ldquo;@rbxts/profileservice\u0026rdquo; \u0026ldquo;@rbxts/replicaservice\u0026rdquo; \u0026ldquo;@rbxts/cmdr\u0026rdquo; \u0026ldquo;@rbxts/testez\u0026rdquo; ‚ÑπÔ∏è Note Confirmed to work\n\u0026ldquo;@rbxts/react\u0026rdquo; \u0026ldquo;@rbxts/react-roblox\u0026rdquo; ‚ÑπÔ∏è Warning CI/CD is not included.\nInstallation üí° Tip You can use this via the \u0026ldquo;Use this template\u0026rdquo; button.\nInstall Packages 1 pnpm install Install VSCode Extensions When you open this project, VSCode will recommend the following extensions. Please install them.\nroblox-ts ESLint Prettier Rojo Usage From VSCode to Rojo 1 pnpm run watch Ctrl + Shift + P\nRojo: Open Menu\n‚ÑπÔ∏è Note If this is your first time, launch Roblox Studio and install the Roblox Studio Plugin.\n‚ñ∂ default.project.json\nFrom Roblox Studio to Rojo \u0026ldquo;Plugins\u0026rdquo; tab\n\u0026ldquo;Rojo\u0026rdquo; ribbon\n\u0026ldquo;Connect\u0026rdquo; button\n","date":"2025-06-22T00:00:00+09:00","image":"https://takoyakisoft.com/p/roblox-ts-template-for-modern-development/roblox-ts-template-for-modern-development_hu_a201222c2d03b0b2.webp","permalink":"https://takoyakisoft.com/en/p/roblox-ts-template-for-modern-development/","title":"A Modern Development Environment Template for Roblox (TypeScript) and VSCode"},{"content":"GitHub Template roblox-rojo-wally-template\nEnglish Êó•Êú¨Ë™û\nWhat is this? This is a template for a modern development environment for Roblox (Luau) using VSCode.\nVSCode üëâ Roblox Studio Sync: Rojo Linter: Selene Formatter: StyLua Package Manager: Wally Manager for Rojo and Wally: Rokit ‚ÑπÔ∏è Warning CI/CD is not included: CI/CD\nInstallation üí° Tip You can use this via the \u0026ldquo;Use this template\u0026rdquo; button.\nInstalling Rokit Windows (PowerShell)\n1 Invoke-RestMethod https://raw.githubusercontent.com/rojo-rbx/rokit/main/scripts/install.ps1 | Invoke-Expression macOS / Linux\n1 curl -fsSL https://raw.githubusercontent.com/rojo-rbx/rokit/main/scripts/install.sh | sh Installing Rojo, Wally, etc. 1 2 3 rokit add rojo rokit add wally rokit add wally-package-types Installing Packages 1 2 3 4 5 wally install rojo sourcemap default.project.json --output sourcemap.json wally-package-types -s sourcemap.json Packages/ wally-package-types -s sourcemap.json ServerPackages/ wally-package-types -s sourcemap.json DevPackages/ Installing VSCode Extensions When you open this project in VSCode, you will be prompted to install the following recommended extensions. Please install them.\nRojo Luau Language Server Selene StyLua Usage Rojo from VSCode Press Ctrl + Shift + P\nSelect Rojo: Open Menu\n‚ÑπÔ∏è Note If this is your first time, launch Roblox Studio and install the Roblox Studio Plugin.\nSelect ‚ñ∂ default.project.json\nRojo from Roblox Studio Go to the \u0026ldquo;Plugins\u0026rdquo; tab.\nIn the \u0026ldquo;Rojo\u0026rdquo; ribbon, click the \u0026ldquo;Connect\u0026rdquo; button.\nAdding Packages with Wally Edit wally.toml in VSCode.\nFind the package you want on wally.run and copy its name from the \u0026ldquo;Install\u0026rdquo; section.\nThe sections are categorized as follows:\n[dependencies] is for both client and server. [server-dependencies] is for server-only. [dev-dependencies] is for development and testing only.\nOnce you\u0026rsquo;ve finished editing, run the package installation steps again.\nReferences How Big Studios Develop on Roblox\n","date":"2025-06-21T00:00:00+09:00","image":"https://takoyakisoft.com/p/roblox-luau-template-for-modern-development/roblox-luau-template-for-modern-development_hu_a201222c2d03b0b2.webp","permalink":"https://takoyakisoft.com/en/p/roblox-luau-template-for-modern-development/","title":"A Modern Development Environment Template for Roblox (Luau) and VSCode"},{"content":"Hello there!\nBy the way, I\u0026rsquo;m even using Rovo Dev CLI\u0026rsquo;s AI assistant feature to write this article. Pretty handy, right?\nToday, I\u0026rsquo;m going to walk you through Atlassian\u0026rsquo;s AI development agent, \u0026ldquo;Rovo Dev CLI.\u0026rdquo;\nIt\u0026rsquo;s a tool similar to the popular \u0026ldquo;Claude Code.\u0026rdquo; You can use it in your terminal to chat with an AI, get help with coding, generate tests, ask for refactoring, and much more. And the best part? It\u0026rsquo;s currently free to use during its beta period.\nHowever, if you try to use it directly in Windows PowerShell, you\u0026rsquo;ll run into issues with garbled Japanese characters\u0026hellip;\nBut don\u0026rsquo;t worry! In this article, I\u0026rsquo;ll cover everything from the basic usage to a slick solution for that garbled text problem using \u0026ldquo;WSL2\u0026rdquo; and \u0026ldquo;VSCode.\u0026rdquo; We\u0026rsquo;ll even dive into an advanced \u0026ldquo;MCP integration\u0026rdquo; technique that lets you command an AI to perform 3D modeling. I\u0026rsquo;ll provide concrete steps for everything.\nWhat is Rovo Dev CLI? Rovo Dev CLI is a command-line AI assistant developed by Atlassian, the company famous for Jira and Confluence.\nWhen I asked Rovo Dev itself, \u0026ldquo;What model are you using?\u0026rdquo;, it told me it uses Anthropic\u0026rsquo;s Claude family.\nFor Windows Users: Start Here! WSL2 Setup If you\u0026rsquo;re using Rovo Dev CLI on Windows, I strongly recommend using WSL2 (Windows Subsystem for Linux 2). As I mentioned earlier, this is because using it directly in PowerShell can lead to garbled Japanese text.\nWSL2 Installation and Configuration Install WSL2 Open \u0026ldquo;PowerShell\u0026rdquo; and run this command.\n1 wsl --install -d Ubuntu-24.04 In my environment, I had to restart my PC for WSL to start up correctly. If you run into trouble, give that a try.\nWSL2 Initial Setup Once the installation is complete, Ubuntu will launch. Set up your username and password.\nSet Up Convenient Mirrored Networking Mode By configuring WSL2\u0026rsquo;s new \u0026ldquo;mirrored\u0026rdquo; mode, the network is shared between Windows and WSL2, allowing you to access localhost from either side. This will be incredibly useful for the MCP integration I\u0026rsquo;ll explain later.\nCreate a file named .wslconfig in your Windows user folder (C:\\Users\\your-username) and add the following content.\n1 2 [wsl2] networkingMode=mirrored You can also do the same from the \u0026ldquo;WSL Settings\u0026rdquo; menu.\nRestart WSL2 To apply the settings, run this command in PowerShell.\n1 wsl --shutdown After that, start WSL2 (Ubuntu) again.\nNow your WSL2 environment is all set! In the next step, we\u0026rsquo;ll connect to this WSL2 environment from VSCode to make things even more comfortable.\nLet\u0026rsquo;s Connect to WSL2 from VSCode From here on out, we\u0026rsquo;ll be doing everything using the VSCode WSL extension.\nVSCode WSL Extension Installation and Connection Steps Install VSCode If you haven\u0026rsquo;t already, download and install it from the official website.\nInstall the WSL Extension Launch VSCode, go to the Extensions tab on the left (Ctrl+Shift+X), search for \u0026ldquo;WSL,\u0026rdquo; and install it.\nConnect to WSL2 Click the green icon in the bottom-left corner of the VSCode window and select \u0026ldquo;Connect to WSL using Distro\u0026hellip;\u0026rdquo;. Select Ubuntu-24.04 Choose the Ubuntu-24.04 distribution we just installed.\nOpen the Terminal Go to the VSCode menu and select \u0026ldquo;Terminal\u0026rdquo; -\u0026gt; \u0026ldquo;New Terminal,\u0026rdquo; or use the shortcut Ctrl+` to open the terminal. You can also open it as shown in the image. Now you\u0026rsquo;re ready to access WSL2\u0026rsquo;s Ubuntu from within VSCode! Let\u0026rsquo;s perform all subsequent operations in this VSCode terminal. Editing and creating files is also a breeze since you can use the VSCode editor.\nInstallation and Initial Setup Install ACLI (Atlassian CLI) Rovo Dev is used via ACLI, Atlassian\u0026rsquo;s common tool. First, let\u0026rsquo;s install it on our WSL2 Ubuntu.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Install required packages sudo apt-get install -y wget gnupg2 # Set up the APT repository # Create directory for the key sudo mkdir -p -m 755 /etc/apt/keyrings # Download the public key and convert it to GPG format wget -nv -O- https://acli.atlassian.com/gpg/public-key.asc | sudo gpg --dearmor -o /etc/apt/keyrings/acli-archive-keyring.gpg # Set permissions for the key file sudo chmod go+r /etc/apt/keyrings/acli-archive-keyring.gpg # Add the repository information to APT\u0026#39;s configuration echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/acli-archive-keyring.gpg] https://acli.atlassian.com/linux/deb stable main\u0026#34; | sudo tee /etc/apt/sources.list.d/acli.list \u0026gt; /dev/null # Install ACLI sudo apt update sudo apt install -y acli If you\u0026rsquo;re using a different OS, you can find instructions on the official Install and update page.\nGet an Atlassian API Token and Authenticate Next, let\u0026rsquo;s get an API token to log in with your Atlassian account.\nGo to your Atlassian ID profile. Click \u0026ldquo;Create API token\u0026rdquo; and create a token with a descriptive name (e.g., rovo-dev-cli). The generated token is only displayed once, so be sure to copy and save it somewhere safe! Once you have the token, run this command in the terminal to authenticate.\n1 acli rovodev auth login Enter your email address and paste the API token you just copied to complete the authentication.\nLaunch! Alright, it\u0026rsquo;s finally time to start the AI agent.\n1 acli rovodev run Running this will switch your terminal to an interactive chat mode with the AI. How exciting!\nGood to Know! Basic Usage Rovo Dev CLI is packed with features that will make your development work much easier.\nCustom Prompts (Custom Instructions) Repeating the same requests every time can be a bit tedious. That\u0026rsquo;s where custom prompts come in handy. You can give the AI background knowledge in advance, similar to the CLAUDE.md file in the Claude desktop app.\nThere are three ways to set this up:\nGlobal Config File Write your instructions in ~/.rovodev/config.yml. These are system-wide instructions. 1 2 3 4 additionalSystemPrompt: | You are an expert in Python and TypeScript. When writing code, always include tests. Provide explanations concisely and in English. Global Agent File Write instructions for the AI to read in a file called ~/.rovodev/.agent.md. Since it\u0026rsquo;s Markdown, you can be more detailed. 1 2 3 4 5 6 7 8 9 10 # Developer Guidelines ## Coding Style - Use 2 spaces for indentation. - Function names should be in camelCase. - Class names should be in PascalCase. ## Testing Policy - Unit tests are mandatory. - Use mocks to speed up tests. Project-Specific Instructions Create an .agent.md file in the root of your current repository. This is useful for writing rules to be shared with your team. For personal settings, I recommend using .agent.local.md, which you can exclude from Git version control. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Project-Specific Guidelines This project is a web application using Next.js and TypeScript. ## Architecture - pages/ - Page components for routing - components/ - Reusable UI components - lib/ - Utility functions and helpers - styles/ - CSS modules and global styles ## Development Rules - Create components as functional components. - Use React Hooks for state management. - Use SWR for API requests. You can combine all these settings, and the AI will consider all instructions when responding. Writing effective custom instructions will help you get higher-quality answers from the AI.\nExample Commands In interactive mode, you can make requests like these:\nsummarize this file ./path/to/file.js: Summarize this file! add unit tests for UserService: Write unit tests for UserService! Refactor this complex function to be more readable: Rewrite this complex function to be more readable! The AI understands your directory structure, so it can grasp the context just by being given a file path, which is pretty smart.\nList of Useful Interactive Mode Commands Interactive mode has many other useful commands to make your interactions with the AI even smoother. If you want to know more about a specific command, just type /command-name help in interactive mode.\nSession Management: /sessions This command manages your sessions. It\u0026rsquo;s incredibly useful because it allows you to separate conversation histories and contexts.\nKey Features: Session Management: Create and switch between multiple conversation sessions. Context Persistence: Each session remembers its own conversation history. Workspace Isolation: You can separate sessions by project. Session Forking: You can even branch off from the current conversation to create a new session. When you restart Rovo Dev CLI with the --restore option, it will automatically restore your previous session.\nClear Session: /clear This command erases the entire conversation history of the current session, resetting it to a blank slate. Be careful, as this action cannot be undone. If you want to keep the history, use the /sessions command to create a new session or the /prune command, which I\u0026rsquo;ll introduce next.\nOptimize Session: /prune Use this when your conversation starts getting long. It\u0026rsquo;s a smart command that saves tokens by trimming the history while keeping the important parts. It removes things like the results of tools (programs) executed by the AI to slim down the history.\nPre-defined Instruction Templates: /instructions This allows you to run pre-defined instruction templates for common tasks like code reviews or documentation writing.\nBuilt-in Instruction Templates: Code review and analysis Documentation generation and improvement Unit test creation and coverage improvement Confluence page summarization Jira issue analysis You can also create your own custom templates!\nCreate an instructions.yml file in .rovodev/. Create a Markdown file with the instruction content inside the .rovodev folder. To use them, just type /instructions to see a list of templates.\nMemory Management: /memory This feature lets you have Rovo Dev CLI remember important information about your projects and settings.\nTypes of Memory: Project Memory: Saved in the current directory (.agent.md and .agent.local.md). User Memory: Saved globally in your home directory (~/.rovodev/agent.md). Memory files are in Markdown format, and they\u0026rsquo;re useful for writing down project rules, coding conventions, and so on. You can quickly add a note by typing something like # Here's something to remember.\nFeedback: /feedback A command for sending feedback or bug reports about Rovo Dev CLI.\nUsage: /usage Check your LLM token usage for the day. Make sure you\u0026rsquo;re not overdoing it!\nExit: /exit Exits the application. /quit or /q also work.\n[Advanced] Integration Between WSL2 and Windows Thanks to the VSCode WSL extension, file sharing and application integration between WSL2 and Windows become incredibly smooth.\nUtilizing Mirrored Networking Mode The WSL2 mirrored networking mode we set up earlier really shines here.\nPort Sharing: You can directly access a server running in WSL2 (e.g., localhost:3000) from a browser on Windows. Shared Network Services: You can use services on the same network from both Windows and WSL2. Integration with MCP Server: This feature is especially crucial for the MCP integration we\u0026rsquo;ll discuss next. It allows Rovo Dev CLI in WSL2 to seamlessly connect to the Blender MCP server running on Windows. This integration lets you get the best of both worlds: the user-friendly interface of Windows and the powerful command line of Linux.\n[Advanced] Let\u0026rsquo;s Make AI Control Blender with MCP! Time for the grand finale! Let\u0026rsquo;s use Rovo Dev\u0026rsquo;s powerful MCP (Model Context Protocol) feature to control Blender on Windows from WSL2. We\u0026rsquo;ll be using a handy open-source tool called BlenderMCP.\nWhat is BlenderMCP? BlenderMCP is a tool that connects AI with Blender. With it, the AI can directly control Blender to perform 3D modeling, create scenes, manipulate objects, and more!\nKey Features:\nBidirectional Communication: Connects AI and Blender. Object Manipulation: Create, move, and delete objects. Material Control: Apply colors and set textures. Scene Inspection: The AI can check the current state of the Blender scene. Code Execution: The AI can send and execute Python code in Blender. Setup Steps BlenderMCP consists of two parts:\nWindows Side: Blender Add-on (addon.py): Installed in Blender on Windows. WSL2 Side: MCP Server: Installed on WSL2\u0026rsquo;s Ubuntu and used by Rovo Dev CLI. Let\u0026rsquo;s get it set up!\nWindows Side: Install the Blender Add-on\nDownload addon.py from the BlenderMCP GitHub repository. Open Blender, go to \u0026ldquo;Edit\u0026rdquo; \u0026gt; \u0026ldquo;Preferences\u0026rdquo; \u0026gt; \u0026ldquo;Add-ons.\u0026rdquo; Click \u0026ldquo;Install\u0026hellip;\u0026rdquo; and select the addon.py file you downloaded. Enable the add-on by checking the box next to \u0026ldquo;Blender MCP.\u0026rdquo; WSL2 Side: Install the uv Package Manager\nIn your WSL2 Ubuntu terminal, run this command: 1 2 # Install the uv package manager curl -LsSf https://astral.sh/uv/install.sh | sh WSL2 Side: Configure Rovo Dev CLI\nLet\u0026rsquo;s use VSCode to edit the configuration file: 1 2 export EDITOR=\u0026#34;code\u0026#34; acli rovodev mcp This will open the configuration file in VSCode. Add the following content. This setting tells Rovo Dev CLI to automatically start BlenderMCP when it launches. 1 2 3 4 5 6 7 8 { \u0026#34;mcpServers\u0026#34;: { \u0026#34;blender\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;uvx\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;blender-mcp\u0026#34;] } } } Windows Side: Configure Connection in Blender\nIn Blender, open the 3D View\u0026rsquo;s sidebar (press the N key if it\u0026rsquo;s not visible). Find the \u0026ldquo;BlenderMCP\u0026rdquo; tab and click \u0026ldquo;Connect to MCP server.\u0026rdquo; WSL2 Side: Control Blender with Rovo Dev CLI!\nIn your VSCode integrated terminal on WSL2, run acli rovodev run. Try asking the AI something like this: 1 Using Blender, please create a cat. And what happens? A cube should appear in your Blender window (running on Windows) and then move upwards, all based on the command from WSL2. This is the power of the mirrored networking we set up earlier. WSL2 recognizes Blender running on Windows as a server on localhost, enabling seamless integration. Amazing!\nMCP Troubleshooting If things aren\u0026rsquo;t working, check these points:\nConnection Issues: Is the Blender add-on enabled? Did you click \u0026ldquo;Connect to MCP server\u0026rdquo; in Blender\u0026rsquo;s MCP tab? WSL2 Mirrored Networking: Double-check that mirrored mode is properly enabled. Try a Restart: If connection errors persist, restarting both Rovo Dev CLI and Blender might be the quickest fix. Command Execution: Do not run the uvx blender-mcp command directly. Rovo Dev CLI runs it automatically based on the configuration file. The most crucial thing is that the network connection between WSL2 and Windows is correctly set up with mirrored mode.\nSummary In this article, we covered a wide range of topics, from the basics of Atlassian\u0026rsquo;s new AI development tool, \u0026ldquo;Rovo Dev CLI,\u0026rdquo; to tips for Windows users to use it comfortably with WSL2, and even an advanced Blender integration using MCP.\nThe initial setup might seem a bit involved, but once your environment is configured, the development experience of receiving powerful AI support without ever leaving your terminal is truly revolutionary.\nRovo Dev CLI is still in beta, so I\u0026rsquo;m excited to see how it will continue to evolve. I hope this article helps you discover a new style of development.\nGive this futuristic tool a try. Happy coding! üéâ\n","date":"2025-06-19T15:05:00+09:00","image":"https://takoyakisoft.com/p/rovo-dev-cli-guide-wsl2-blender/rovo-dev-cli-guide-wsl2-blender_hu_79a0b353e74edfc1.webp","permalink":"https://takoyakisoft.com/en/p/rovo-dev-cli-guide-wsl2-blender/","title":"A Practical Guide to Rovo Dev CLI: How to Use It Comfortably with VSCode and WSL2"},{"content":"Hello! Today, I\u0026rsquo;ll share my experience using a library called mastra to run LLMs like Grok-2 and Gemini 2.5 Pro exp.\nmastra is a library that lets you decide which LLM to use for a prompt, create \u0026ldquo;AI agents,\u0026rdquo; and then set up a \u0026ldquo;workflow\u0026rdquo; defining the order in which these agents run. The fact that it\u0026rsquo;s written in TypeScript is personally a big plus for me. I\u0026rsquo;m thinking it might allow for affordable integration into web services in environments like Cloudflare Workers or Deno.\nSimilar libraries like LangChain are well-known, but mastra might have a slight advantage in terms of deployment ease. Also, I previously tried a tool called Dify, which seemed a bit weak with iterative processing (loops), so I wanted to see how mastra handles that. That\u0026rsquo;s part of the reason I decided to give it a try.\nAlright, let\u0026rsquo;s dive right in!\nInstalling mastra This time, I\u0026rsquo;m using Windows 11, the AI editor \u0026ldquo;Trae,\u0026rdquo; and the fast package manager \u0026ldquo;pnpm.\u0026rdquo;\nFirst, let\u0026rsquo;s create a project following the steps on the official mastra website. (Reference: Create a New Project)\nOpen your terminal and run the following command:\n1 pnpm create mastra@latest You\u0026rsquo;ll be asked a few questions. Let\u0026rsquo;s answer them.\nIt asks for the project name. I chose my-mastra-app this time.\n1 2 ‚óá What do you want to name your project? ‚îÇ my-mastra-app This is where the source files will be placed. The default src/ is fine.\n1 2 ‚óÜ Where should we create the Mastra files? (default: src/) ‚îÇ src/ Choose the necessary components. We\u0026rsquo;ll use Agents, Workflows, and Tools later, so let\u0026rsquo;s select Yes for them.\n1 2 3 4 5 ‚óÜ Choose components to install: ‚îÇ ‚óº Agents ‚îÇ ‚óº Workflows ‚óá Add tools? ‚îÇ Yes Select the default LLM provider. I chose Google this time, but you can change this freely later.\n1 2 ‚óá Select default provider: ‚îÇ Google API key setup. We\u0026rsquo;ll set this in the .env.development file later, so skipping for now (Skip for now) is okay.\n1 2 ‚óÜ Enter your google API key? ‚îÇ ‚óè Skip for now (default) Add examples? Yes, for this time.\n1 2 ‚óá Add example ‚îÇ Yes AI IDE integration? Skipped for now.\n1 2 ‚óá Make your AI IDE into a Mastra expert? (installs Mastra docs MCP server) ‚îÇ Skip for now Now the project template is created! Navigate to the created project folder and install the necessary libraries.\n1 2 cd my-mastra-app pnpm i Let\u0026rsquo;s start the development server. You can stop it with Ctrl+C.\n1 pnpm run dev Let\u0026rsquo;s look at the configuration Once the installation is complete, let\u0026rsquo;s examine a few files.\nLooking at package.json, you can see @ai-sdk/google in the dependencies. This is a library provided by Vercel, and it seems to support not only Gemini but also other LLM providers like DeepSeek, Grok, and OpenRouter. Looks convenient! (Reference: AI SDK Providers)\nThe scripts section only has dev. Perhaps build or test scripts will be added in the future.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \u0026#34;name\u0026#34;: \u0026#34;my-mastra-app\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;echo \\\u0026#34;Error: no test specified\\\u0026#34; \u0026amp;\u0026amp; exit 1\u0026#34;, \u0026#34;dev\u0026#34;: \u0026#34;mastra dev\u0026#34; }, \u0026#34;keywords\u0026#34;: [], \u0026#34;author\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;module\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;@ai-sdk/google\u0026#34;: \u0026#34;^1.2.5\u0026#34;, \u0026#34;@mastra/core\u0026#34;: \u0026#34;^0.7.0\u0026#34;, \u0026#34;mastra\u0026#34;: \u0026#34;^0.4.4\u0026#34;, \u0026#34;zod\u0026#34;: \u0026#34;^3.24.2\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;@types/node\u0026#34;: \u0026#34;^22.14.0\u0026#34;, \u0026#34;tsx\u0026#34;: \u0026#34;^4.19.3\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^5.8.2\u0026#34; } } This time, I also want to use xAI\u0026rsquo;s Grok, so let\u0026rsquo;s add the corresponding library.\n1 pnpm add @ai-sdk/xai If you no longer need it, you can remove it with pnpm remove @ai-sdk/xai.\nNext is the API key configuration. Create .env.development and .env files in the project root and write the respective API keys. (.env is for production, .env.development is for development).\n.env.development:\n1 2 GOOGLE_GENERATIVE_AI_API_KEY=your-google-api-key XAI_API_KEY=your-xai-api-key If you want to change the LLM model, specify it in the file defining the agent (e.g., src/mastra/agents/index.ts) or the workflow file (e.g., src/mastra/workflows/index.ts).\nFor example, here\u0026rsquo;s how to use Grok-2:\nExample src/mastra/agents/index.ts:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import { xai } from \u0026#39;@ai-sdk/xai\u0026#39;; // Import the xAI library import { Agent } from \u0026#39;@mastra/core/agent\u0026#39;; import { weatherTool } from \u0026#39;../tools\u0026#39;; // Also import the tool to be used export const weatherAgent = new Agent({ name: \u0026#39;Weather Agent\u0026#39;, instructions: ` You are a helpful weather assistant that provides accurate weather information. Your primary function is to help users get weather details for specific locations. When responding: - Always ask for a location if none is provided - If the location name isn‚Äôt in English, please translate it - If giving a location with multiple parts (e.g. \u0026#34;New York, NY\u0026#34;), use the most relevant part (e.g. \u0026#34;New York\u0026#34;) - Include relevant details like humidity, wind conditions, and precipitation - Keep responses concise but informative Use the weatherTool to fetch current weather data. `, model: xai(\u0026#39;grok-2-latest\u0026#39;), // Specify the model here! tools: { weatherTool }, }); You can specify it similarly within a workflow.\nExample src/mastra/workflows/index.ts:\n1 2 3 4 5 6 import { xai } from \u0026#39;@ai-sdk/xai\u0026#39;; import { Agent } from \u0026#39;@mastra/core/agent\u0026#39;; import { Step, Workflow } from \u0026#39;@mastra/core/workflows\u0026#39;; import { z } from \u0026#39;zod\u0026#39;; const llm = xai(\u0026#39;grok-2-latest\u0026#39;); // Define the model to use here Let\u0026rsquo;s check if it works Once the configuration is done, let\u0026rsquo;s start the development server again.\n1 pnpm run dev Access http://localhost:4111/ in your browser, and the mastra interface should appear.\nYou can select an agent and test it in a chat format.\nWhen using Grok-2, the answers might come back in English.\nImplementing the \u0026ldquo;think tool\u0026rdquo; Next, let\u0026rsquo;s implement an interesting tool: the \u0026ldquo;think tool.\u0026rdquo;\nThis is a technique introduced in an Anthropic article, suggesting that adding a \u0026ldquo;thinking\u0026rdquo; step before letting the LLM execute something can improve performance. (Reference: The \u0026ldquo;think\u0026rdquo; tool: Enabling Claude to stop and think in complex tool use situations)\nThis tool itself doesn\u0026rsquo;t fetch external information or anything; it just mimics the process of \u0026ldquo;thinking.\u0026rdquo; However, it seems to be effective in cases requiring complex reasoning. Interesting, isn\u0026rsquo;t it?\nLet\u0026rsquo;s create the think tool with mastra.\nFirst, create a file to define the tool.\nsrc/mastra/tools/thinkTool.ts:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import { createTool } from \u0026#39;@mastra/core/tools\u0026#39;; import { z } from \u0026#39;zod\u0026#39;; // For input data validation export const thinkTool = createTool({ id: \u0026#39;think\u0026#39;, // Tool ID description: \u0026#39;Use the tool to think about something. It will not obtain new information or change the database, but just append the thought to the log. Use it when complex reasoning or some cache memory is needed.\u0026#39;, // Tool description inputSchema: z.object({ // Definition of the input the tool receives thought: z.string().describe(\u0026#39;A thought to think about.\u0026#39;), }), outputSchema: z.object({}), // Definition of the data the tool outputs (empty this time) execute: async ({ context }) =\u0026gt; { // Doesn\u0026#39;t actually do anything console.log(\u0026#39;Thinking:\u0026#39;, context.thought); // Maybe log the thought content to the console return {}; }, }); Next, create an agent that uses this think tool. The key is to instruct the agent in the prompt when to use the think tool.\nsrc/mastra/agents/thinkAgent.ts:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import { Agent } from \u0026#39;@mastra/core/agent\u0026#39;; import { thinkTool } from \u0026#39;../tools/thinkTool\u0026#39;; // Import the created thinkTool import { xai } from \u0026#39;@ai-sdk/xai\u0026#39;; // Also import the LLM model to use export const thinkAgent = new Agent({ name: \u0026#39;Think Agent\u0026#39;, instructions: ` ## Using the think tool Before taking any action or responding to the user after receiving tool results, use the think tool as a scratchpad to: - List the specific rules that apply to the current request - Check if all required information is collected - Verify that the planned action complies with all policies - Iterate over tool results for correctness Here are some examples of what to iterate over inside the think tool: \u0026lt;think_tool_example_1\u0026gt; User wants to [specific scenario] - Need to verify: [key information] - Check relevant rules: [list rules] - Verify [important conditions] - Plan: [outline steps] \u0026lt;/think_tool_example_1\u0026gt; `, model: xai(\u0026#39;grok-2-latest\u0026#39;), // Let\u0026#39;s try Grok-2 here too tools: { thinkTool } // Register this as a tool used by this agent }); Finally, register the created tool and agent with mastra.\nsrc/mastra/index.ts:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import { Mastra } from \u0026#39;@mastra/core/mastra\u0026#39;; import { createLogger } from \u0026#39;@mastra/core/logger\u0026#39;; import { weatherWorkflow } from \u0026#39;./workflows\u0026#39;; import { weatherAgent } from \u0026#39;./agents\u0026#39;; import { thinkAgent } from \u0026#39;./agents/thinkAgent\u0026#39;; // Import the created thinkAgent export const mastra = new Mastra({ workflows: { weatherWorkflow }, // Existing workflow agents: { weatherAgent, thinkAgent }, // Add the agent logger: createLogger({ name: \u0026#39;Mastra\u0026#39;, level: \u0026#39;info\u0026#39;, }), }); That completes the implementation of the think tool!\nChecking the think tool\u0026rsquo;s operation Let\u0026rsquo;s start the development server again and check.\n1 pnpm run dev Access http://localhost:4111/ and this time, select the \u0026ldquo;Think Agent.\u0026rdquo;\nWhen you give it an instruction, the think tool should run behind the scenes before providing a response.\nWith this, it might now be able to think more accurately, even for slightly complex tasks!\nImplementing a Workflow to Create a Prompt that Clones Writing Style Alright, now let\u0026rsquo;s tackle a more advanced workflow that combines mastra\u0026rsquo;s loop and evaluation features!\nGoal of this Workflow: Teaching AI Your \u0026ldquo;Writing Style\u0026rdquo; The theme is \u0026ldquo;Making the AI itself create an AI prompt that mimics the writing style of a given text.\u0026rdquo; Doesn\u0026rsquo;t it sound interesting, like making AI figure out how to use AI?\nLately, we often hear things like, \u0026ldquo;I can\u0026rsquo;t tell if this was written by AI or a human!\u0026rdquo; If that\u0026rsquo;s the case, maybe if we could teach AI the quirks of our own writing style and have it generate blog post drafts, it would reduce typing and make things easier? That thought was the inspiration for creating this workflow.\nPreparation: Adding the Evaluation Library and Registering the Workflow First, preparation: Add the evaluation library\nThis workflow uses mastra\u0026rsquo;s evaluation library to have another AI assess the quality of the generated prompts. Install it by running the following in your terminal:\n1 pnpm add @mastra/evals Register the workflow with mastra\nNext, update the configuration file (src/mastra/index.ts) to make mastra aware of the workflow we\u0026rsquo;re about to create (clonePromptGeneratorWorkflow).\nsrc/mastra/index.ts:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import { Mastra } from \u0026#39;@mastra/core/mastra\u0026#39;; import { createLogger } from \u0026#39;@mastra/core/logger\u0026#39;; import { weatherWorkflow } from \u0026#39;./workflows\u0026#39;; // ‚Üì Import the new workflow import { clonePromptGeneratorWorkflow } from \u0026#39;./workflows/clonePromptGeneratorWorkflow\u0026#39;; import { weatherAgent } from \u0026#39;./agents\u0026#39;; import { thinkAgent } from \u0026#39;./agents/thinkAgent\u0026#39;; export const mastra = new Mastra({ // ‚Üì Add the new workflow to the workflows object workflows: { weatherWorkflow, clonePromptGeneratorWorkflow }, agents: { weatherAgent, thinkAgent }, logger: createLogger({ name: \u0026#39;Mastra\u0026#39;, level: \u0026#39;info\u0026#39;, }), }); Now we\u0026rsquo;re ready!\nHow the AI Improves Prompts: Explaining the Process Flow Let\u0026rsquo;s look at the actual flow of how the AI generates and improves the prompt.\nPrompt Creator Agent: First, it analyzes the \u0026ldquo;example\u0026rdquo; text you provide (originalText). It identifies the writer\u0026rsquo;s characteristics (persona, style, common phrases, etc.) and creates the initial version of an \u0026ldquo;impersonation instruction prompt\u0026rdquo; telling another AI, \u0026ldquo;Write like this person!\u0026rdquo;\nTheme Abstractor Agent: Next, it removes specific proper nouns (like mastra, Grok-2, etc.) from the example text and extracts a general \u0026ldquo;abstracted theme,\u0026rdquo; such as \u0026ldquo;Steps for creating an AI processing flow using a software toolkit.\u0026rdquo; This is to prevent the impersonation test from generating the exact same content as the example.\nText Generator Agent: Using the \u0026ldquo;impersonation instruction prompt\u0026rdquo; from step 1 and the \u0026ldquo;abstracted theme\u0026rdquo; extracted in step 2, it actually generates impersonated text. It\u0026rsquo;s like saying, \u0026ldquo;Write about the \u0026lsquo;abstracted theme\u0026rsquo; following the impersonation instruction prompt.\u0026rdquo;\nEvaluation AI (Authorship Similarity Judge/Metric): Now for the evaluation. It compares the example text with the impersonated text generated by the AI in step 3 and scores the \u0026ldquo;writing style similarity\u0026rdquo; from 0.0 (not similar at all) to 1.0 (identical!). It only looks at the \u0026ldquo;writing style\u0026rdquo;‚Äîword choice, sentence length, tone, punctuation usage, etc.‚Äîand doesn\u0026rsquo;t assess the content\u0026rsquo;s correctness.\nLoop Decision \u0026amp; Feedback:\nIf the evaluation score exceeds a predefined threshold (set as SIMILARITY_THRESHOLD = 0.7 in the code), it\u0026rsquo;s a \u0026ldquo;pass!\u0026rdquo; The workflow ends and outputs the successful \u0026ldquo;impersonation instruction prompt.\u0026rdquo; If the score is below the threshold, it\u0026rsquo;s \u0026ldquo;needs improvement.\u0026rdquo; The evaluation AI provides feedback explaining why the score is low (e.g., \u0026ldquo;punctuation usage is not similar enough,\u0026rdquo; \u0026ldquo;tone is too formal\u0026rdquo;). This feedback is sent back to the Prompt Creator Agent in step 1 with the instruction, \u0026ldquo;Use this feedback to create a better prompt!\u0026rdquo; and the process loops. By repeating this loop, the AI iteratively improves the \u0026ldquo;impersonation instruction prompt\u0026rdquo; through trial and error, getting closer to generating text that matches the example\u0026rsquo;s writing style.\nTips:\nModel Selection: The code uses xai('grok-2-latest'), but if you have access to more powerful models like gemini('gemini-2.5-pro-exp-03-25') (e.g., via Google AI Studio), try swapping it in the model: llm parts of each Agent and Metric. You might get much better results in a single loop! Similarity Score Threshold: The SIMILARITY_THRESHOLD value (0.7) determines how strict the evaluation is. If the loop doesn\u0026rsquo;t seem to end, try lowering it slightly. If you want higher accuracy, try raising it. Adjust it manually as needed. Let\u0026rsquo;s Run It! Workflow Execution Steps With the development server running (pnpm run dev), access http://localhost:4111/ in your browser.\nSelect \u0026ldquo;Workflows\u0026rdquo; from the left menu and choose the clone-prompt-generator-workflow-with-eval workflow we created.\nYou should see an input field labeled OriginalText in the \u0026ldquo;Run\u0026rdquo; tab on the right. Paste the text you want the AI to mimic (e.g., part of a blog post you previously wrote).\nAfter pasting the text, click the \u0026ldquo;Submit\u0026rdquo; button!\nNow, just watch the logs flowing in your terminal (or the OUTPUT panel in VS Code, etc.). You should see the evaluation score and feedback displayed with each loop iteration.\nComplete Workflow Code Here is the code for the complete working workflow:\nsrc/mastra/workflows/clonePromptGeneratorWorkflow.ts:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 import { xai } from \u0026#39;@ai-sdk/xai\u0026#39;; // or use gemini, openai, etc. // import { gemini } from \u0026#39;@ai-sdk/google\u0026#39;; // Example for using Gemini import { Agent } from \u0026#39;@mastra/core/agent\u0026#39;; import { Step, Workflow } from \u0026#39;@mastra/core/workflows\u0026#39;; import { z } from \u0026#39;zod\u0026#39;; import { type LanguageModel } from \u0026#39;@mastra/core/llm\u0026#39;; import { MastraAgentJudge } from \u0026#39;@mastra/evals/judge\u0026#39;; import { Metric, type MetricResult } from \u0026#39;@mastra/core/eval\u0026#39;; // --- Configuration --- // LLM model to use (change as needed) const llm = xai(\u0026#39;grok-2-latest\u0026#39;); // const llm = gemini(\u0026#39;gemini-2.5-pro-exp-03-25\u0026#39;); // Example for using Gemini 2.5 Pro Experimental // Similarity score threshold to determine if it\u0026#39;s the same author (adjustable) const SIMILARITY_THRESHOLD = 0.7; // Variable to store feedback for the Prompt Creator Agent let feedbackForPromptCreator = \u0026#34;\u0026#34;; // Loop counter let iteration = 0; // --- Evaluation Related Definitions --- /** * Function to generate the prompt for the evaluation AI */ const generateSimilarityPrompt = ({ originalText, generatedText, }: { originalText: string; generatedText: string; }) =\u0026gt; ` You are an expert in comparative analysis of writing styles. Compare the provided \u0026#34;Example Text\u0026#34; and \u0026#34;AI Generated Text\u0026#34; and evaluate whether they appear to be written by the **same person**. **Evaluation Criteria:** Focus on the following elements to determine the overall similarity in writing style: * **Style:** First-person usage, tone (polite, casual, etc.), sentence endings (~desu, ~da, ~yone, etc. - consider equivalent English indicators like formality, slang) * **Word Choice:** Preferred words, phrasing, frequency of technical terms * **Text Structure:** Sentence length, paragraph usage, conjunction usage, logical flow * **Rhythm/Tempo:** Punctuation usage, frequency of nominalization (or similar stylistic choices in English) * **Emotional Expression:** Positive/negative sentiment, way of expressing emotions, presence of humor * **Quirks:** Characteristic phrases, tendency for typos (if any) **Important:** Evaluate only the similarity in **writing style**, not the topic or correctness of the content. **Output Format:** Return the evaluation results in the following JSON format: * \\`similarityScore\\`: A **numerical** score for writing style similarity between 0.0 (not similar at all) and 1.0 (looks exactly like the same person). * \\`reason\\`: Briefly explain why you gave that score. If the score is low, point out specifically which aspects felt different. \\`\\`\\`json { \u0026#34;similarityScore\u0026#34;: number (0.0 ~ 1.0), \u0026#34;reason\u0026#34;: string } \\`\\`\\` --- **Example Text:** \\`\\`\\` ${originalText} \\`\\`\\` --- **AI Generated Text:** \\`\\`\\` ${generatedText} \\`\\`\\` --- Follow the format above and output the evaluation result in JSON. `; /** * Type definition for the evaluation result (zod schema) */ const SimilarityEvaluationSchema = z.object({ similarityScore: z.number().min(0).max(1).describe(\u0026#34;Writing style similarity score (0.0 to 1.0)\u0026#34;), reason: z.string().describe(\u0026#34;Reason for the evaluation\u0026#34;), }); type SimilarityEvaluation = z.infer\u0026lt;typeof SimilarityEvaluationSchema\u0026gt;; /** * Evaluator (Judge) Class */ class AuthorshipSimilarityJudge extends MastraAgentJudge { constructor(model: LanguageModel) { super( \u0026#39;Authorship Similarity Judge\u0026#39;, \u0026#39;You are an expert in comparative analysis of writing styles. Follow the given instructions to evaluate the similarity between two texts.\u0026#39;, model ); } async evaluate(originalText: string, generatedText: string): Promise\u0026lt;SimilarityEvaluation\u0026gt; { const prompt = generateSimilarityPrompt({ originalText, generatedText }); // Utilize JSON mode or schema enforcement if supported by the model try { const result = await this.agent.generate(prompt, { output: SimilarityEvaluationSchema }); return result.object; } catch (error) { console.error(\u0026#34;Failed to parse evaluation result. Returning raw text.\u0026#34;, error); // Fallback: If JSON parsing fails, handle it (e.g., return score 0) const fallbackResult = await this.agent.generate(prompt); return { similarityScore: 0.0, reason: `Invalid output format from evaluation AI: ${fallbackResult.text}` }; } } } /** * Evaluation Metric Class */ interface AuthorshipSimilarityMetricResult extends MetricResult { info: SimilarityEvaluation; } class AuthorshipSimilarityMetric extends Metric { private judge: AuthorshipSimilarityJudge; constructor(model: LanguageModel) { super(); this.judge = new AuthorshipSimilarityJudge(model); } async measure(originalText: string, generatedText: string): Promise\u0026lt;AuthorshipSimilarityMetricResult\u0026gt; { const evaluationResult = await this.judge.evaluate(originalText, generatedText); return { score: evaluationResult.similarityScore, info: evaluationResult, }; } } // --- Workflow Definition --- const clonePromptGeneratorWorkflow = new Workflow({ name: \u0026#34;clone-prompt-generator-workflow-with-eval\u0026#34;, // Workflow name triggerSchema: z.object({ originalText: z.string().describe(\u0026#34;The original text written by the user to be cloned.\u0026#34;), }), }); // --- Step 1: Generate Impersonation Instruction Prompt --- const generateClonePromptStep = new Step({ id: \u0026#34;generate-clone-prompt\u0026#34;, execute: async ({ context }) =\u0026gt; { const promptCreatorAgent = new Agent({ name: \u0026#39;Prompt Creator Agent\u0026#39;, instructions: ` # Instruction: Create the best \u0026#34;Impersonation Text Generation Prompt\u0026#34; You are a Prompt Engineer AI. Your mission is to analyze the provided **[Example Text]**, accurately capture the writer\u0026#39;s **characteristics (personality, writing style, quirks)**, and create a **\u0026#34;General-purpose Instruction Prompt\u0026#34;** that enables another AI to generate **human-like, natural text** impersonating that writer. (Omitted... conditions and thought process for the prompt remain the same) **Now, considering all the above, create the best \u0026#34;Impersonation Text Generation Prompt\u0026#34;.** ${feedbackForPromptCreator} `, model: llm, }); const originalText = context.triggerData.originalText; console.log(\u0026#34;[generate-clone-prompt] Starting prompt generation.\u0026#34;); const result = await promptCreatorAgent.generate(` [Example Text]: ${originalText} `); console.log(\u0026#34;[generate-clone-prompt] Prompt generation complete.\u0026#34;); return `${result.text}`; }, }); // --- Final Step: Output the Successful Prompt --- const outputFinalPromptStep = new Step({ id: \u0026#34;output-final-prompt\u0026#34;, execute: async ({ context }) =\u0026gt; { const finalPrompt = context.getStepResult(generateClonePromptStep) as string; console.log(\u0026#34;--------------------------------------------------\u0026#34;); console.log(\u0026#34;Workflow completed successfully!\u0026#34;); console.log(\u0026#34;Successful Impersonation Instruction Prompt:\u0026#34;); console.log(\u0026#34;--------------------------------------------------\u0026#34;); console.log(finalPrompt); // Output the final prompt to the console console.log(\u0026#34;--------------------------------------------------\u0026#34;); return finalPrompt; }, }); // --- Assembling the Workflow --- clonePromptGeneratorWorkflow .step(generateClonePromptStep) .until(async ({ context }) =\u0026gt; { // Receives iteration iteration++; console.log(`\\n--- Loop ${iteration} Start ---`); // 1. Get the generated \u0026#34;Impersonation Instruction Prompt\u0026#34; const generatedClonePrompt = context.getStepResult(generateClonePromptStep) as string; const originalText = context.triggerData.originalText; // 2. Abstract the theme of originalText const themeAbstractorAgent = new Agent({ name: \u0026#39;Theme Abstractor Agent\u0026#39;, instructions: ` Analyze the main theme or topic of the given text and express it in more general and abstract terms, **avoiding specific proper nouns, product names, technology names, service names, etc.** The output should be only the description of the abstracted theme, without any other explanations or introductions. Example: (Omitted...) `, model: llm, }); console.log(`[Loop ${iteration}] Abstracting theme from originalText...`); const abstractionResult = await themeAbstractorAgent.generate(` Abstract the theme of the following text:\\n---\\n${originalText}\\n--- `); const abstractedTheme = abstractionResult.text.trim(); console.log(`[Loop ${iteration}] Abstracted theme: ${abstractedTheme}`); // 3. Generate impersonated text on the abstracted theme const textGeneratorAgent = new Agent({ name: \u0026#39;Text Generator Agent\u0026#39;, instructions: generatedClonePrompt, model: llm, }); console.log(`[Loop ${iteration}] Generating text on the abstracted theme...`); const generatedTextResult = await textGeneratorAgent.generate( `Create an explanation about \u0026#34;${abstractedTheme}\u0026#34; that is easy for beginners to understand.` ); const generatedText = generatedTextResult.text; // 4. Evaluate similarity console.log(`[Loop ${iteration}] Evaluating similarity between generated text and example text...`); const authorshipMetric = new AuthorshipSimilarityMetric(llm); // Use the evaluation metric const metricResult = await authorshipMetric.measure( originalText, generatedText ); const currentScore = metricResult.score; const reason = metricResult.info.reason; console.log(`[Loop ${iteration}] Evaluation Result - Score: ${currentScore.toFixed(2)}, Reason: ${reason}`); // 5. Decide whether to continue or stop the loop based on the evaluation score const shouldStop = currentScore \u0026gt;= SIMILARITY_THRESHOLD; if (!shouldStop) { feedbackForPromptCreator = ` --- **Feedback from previous attempt (Loop ${iteration}):** The result of writing about \u0026#34;${abstractedTheme}\u0026#34; using the generated \u0026#34;Impersonation Instruction Prompt\u0026#34; did not meet the target similarity score with the example text (Score: ${currentScore.toFixed(2)}). The evaluation AI pointed out the following issues. Please improve the prompt based on this feedback. **Failed Prompt:** \\`\\`\\` ${generatedClonePrompt} \\`\\`\\` **Evaluation AI\u0026#39;s Comments:** \\`\\`\\` ${reason} \\`\\`\\` --- `; console.log(`[Loop ${iteration}] Score is below threshold. Creating feedback and retrying.`); } else { console.log(`[Loop ${iteration}] Score is above threshold (${SIMILARITY_THRESHOLD}). Stopping loop.`); feedbackForPromptCreator = \u0026#34;\u0026#34;; // Clear feedback on success } console.log(`--- Loop ${iteration} End ---`); return shouldStop; // true stops the loop, false continues }, generateClonePromptStep) // Specify the step(s) to loop over .then(outputFinalPromptStep) // Specify the step to execute after the loop ends .commit(); // Finalize the workflow definition export { clonePromptGeneratorWorkflow }; Done! The \u0026ldquo;Impersonation Instruction Prompt\u0026rdquo; and How to Use It Final Output\nWhen the workflow successfully achieves a score above the threshold and finishes, the final, perfected \u0026ldquo;Impersonation Instruction Prompt\u0026rdquo; will be printed to your terminal console.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 -------------------------------------------------- Workflow completed successfully! Final Similarity Score: 0.75 (Example) Successful Impersonation Instruction Prompt: -------------------------------------------------- # Instruction Prompt You are an AI that generates human-like, natural text based on [Article Content], impersonating a person with the following characteristics. ## Characteristics of the Person to Impersonate: ### Personality: * Basic stance: Polite and friendly. Strives to convey information efficiently. * Speaking style: Gentle demeanor. Uses expressions like \u0026#34;It\u0026#39;s kind of like...\u0026#34;, but explains procedures briskly. * Thinking: Logical yet flexible. Practical. ... (The rest of the AI-generated prompt follows) ... -------------------------------------------------- You can copy this outputted prompt and use it for future text generation (even in other tools like ChatGPT or Claude). This should allow you to have the AI create drafts that mimic your writing style!\nConclusion This time, we used mastra\u0026rsquo;s loop and evaluation features to create a workflow that teaches an AI your writing style and automatically generates and refines the prompt needed for that impersonation.\nIt was a slightly meta approach‚Äîmaking AI figure out how to use AI‚Äîbut if it works well, it could lead to more efficient writing. It might be particularly worthwhile for those who want to write blog posts or reports in their own style but find the typing laborious.\nmastra is quite a deep and interesting library! I encourage you all to try building various workflows with it.\n","date":"2025-04-03T21:00:34+09:00","image":"https://takoyakisoft.com/p/build-llm-workflow-with-mastra/build-llm-workflow-with-mastra_hu_cd3ccd21a5d0f00f.webp","permalink":"https://takoyakisoft.com/en/p/build-llm-workflow-with-mastra/","title":"Master LLM Workflows with mastra! From Grok-2 and think Tools to Generating Article Cloning Prompts"},{"content":"Hello!\nToday, I\u0026rsquo;d like to share my experience trying to develop a simple web game using some incredibly powerful AI models that are currently available for free (as of March 30, 2025).\nAmazingly, Google\u0026rsquo;s experimental model of \u0026lsquo;Gemini 2.5 Pro\u0026rsquo; (gemini-2.5-pro-exp-03-25) is available for free on the website ai.dev! It\u0026rsquo;s said to be one of the state-of-the-art (SOTA) models currently available, which is quite impressive!\nFurthermore, I also tried the \u0026lsquo;Builder\u0026rsquo; feature (beta version) in the editor called \u0026lsquo;Trae\u0026rsquo; by ByteDance. Here, you can use an AI agent similar to Claude, and you can even select \u0026lsquo;Claude 3.7 Sonnet\u0026rsquo; as the model. This is also available for free.\nHowever, since both are free plans, it\u0026rsquo;s likely that the data you input might be used for service improvements. Therefore, they might not be suitable for development involving confidential information, but they could be perfect for programs intended for public release or personal experiments!\nHaving AI Create a Game with Three.js So, this time, I challenged myself to have these powerful AIs, especially \u0026lsquo;Gemini 2.5 Pro\u0026rsquo;, assist in creating a game using the JavaScript 3D library Three.js, contained within just a single HTML file.\nInstead of just generating the code, I communicated the concept of \u0026lsquo;juiciness\u0026rsquo; to the AI and requested several revisions to make the game more engaging. \u0026lsquo;Juiciness\u0026rsquo; refers to elements that enhance the feel of gameplay, such as satisfying controls and flashy effects.\nThe Resulting Game And this is the game that was created!\nControls:\nMove the mouse cursor to change the camera\u0026rsquo;s viewpoint. Click the mouse to fire projectiles. Press the ESC key to release mouse focus. It\u0026rsquo;s quite simple, but you might feel a bit of \u0026lsquo;juiciness\u0026rsquo; in the responsiveness to mouse movements and the effects.\nGeneration Time and Points to Note This game is actually self-contained within a single index.html file, and the code amounts to approximately 1500 lines.\nWhen generating this with Gemini 2.5 Pro (the gemini-2.5-pro-exp-03-25 model), it took about 180 seconds to output. That\u0026rsquo;s quite a bit of time.\nBased on this experience, it seems that aiming for around 1000 lines might be a good guideline when asking AI to generate code contained within a single file, in order to proceed with work within a realistic timeframe.\nOf course, this can vary depending on the model\u0026rsquo;s performance and server load, but please consider it as a general reference point.\nSummary In this article, I introduced my attempt to generate a Three.js game using the latest free AI models like \u0026lsquo;Gemini 2.5 Pro\u0026rsquo; and \u0026lsquo;Trae Builder (Claude 3.7 Sonnet)\u0026rsquo;.\nIt\u0026rsquo;s truly surprising that AI with such capabilities is available for free, and it seems like various things can be created depending on the ideas! I believe these are very useful tools, especially for projects intended for public release or for learning purposes.\nWhile there might still be some quirks, such as the code generation time, I felt it was definitely worth trying.\nThank you for reading this far! I encourage you all to give it a try.\n","date":"2025-03-30T23:43:05+09:00","image":"https://takoyakisoft.com/p/free-ai-models-create-threejs-game/free-ai-models-create-threejs-game_hu_6af50b4fb5b6c13f.webp","permalink":"https://takoyakisoft.com/en/p/free-ai-models-create-threejs-game/","title":"Trying to Develop a Three.js Game Using the Latest Free AI Models Gemini 2.5 Pro and Trae Builder!"},{"content":"Introduction Hello!\nYou\u0026rsquo;ve created your own game with Pygame, and naturally, you want to publish it on the web so many people can play it, right? I feel the same way!\nIn this article, we\u0026rsquo;ll explore how to make games created with the Python game library Pygame easily playable by anyone directly in a web browser. I\u0026rsquo;ll try to explain it as clearly as possible.\nWhat you\u0026rsquo;ll learn in this article:\nHow to use a tool called pygbag to convert (build) your Pygame game for the web using WebAssembly (WASM). This is quite convenient. The steps to embed the converted game into a blog created with the static site generator Hugo. How to create a Hugo shortcode to easily embed the game into your posts. Making this will make things easier later on. With these steps, you can make your game playable directly in the browser without needing any special server setup. Isn\u0026rsquo;t that great! Your game might be played by people all over the world!\nThe Finished Game So, here\u0026rsquo;s the game I actually made using this method! (It\u0026rsquo;s a game you just watch.)\nBuilding Your Pygame Game for the Web: Using the Handy Tool pygbag pygbag is a truly useful tool that packages Pygame games so they can run directly in a web browser. Let\u0026rsquo;s use it to convert our game for the web first.\n1. First, Prepare Your Project with uv Here, we\u0026rsquo;ll use uv, a recently popular Python package management tool. If you don\u0026rsquo;t have uv, install it first. (If you\u0026rsquo;re using another tool like pip, that\u0026rsquo;s fine too).\nInstallation command for uv (if needed, this is for Windows) 1 powershell -ExecutionPolicy ByPass -c \u0026#34;irm https://astral.sh/uv/install.ps1 | iex\u0026#34; Next, use uv init to create and initialize a folder for your game project. the-labyrinth-of-gaze is the example game name used here, so please replace it with your own project name.\nInitialize the project with uv 1 2 uv init the-labyrinth-of-gaze cd the-labyrinth-of-gaze Then, install the necessary libraries pygbag and pygame. With uv, it looks like this:\n1 uv add pygbag pygame (If you\u0026rsquo;re not using uv, please install them according to your environment, perhaps using pip install pygbag pygame)\n2. Now, Let\u0026rsquo;s Build the Game Write your Pygame game code in the main.py file automatically created by uv init, or in your game\u0026rsquo;s main script file.\nThe example game code used in this article is attached at the end.\nOnce ready, run the following command to build the game for the web. Replace main.py with the filename of your game\u0026rsquo;s main script.\n1 uv run pygbag --build .\\main.py If this command succeeds, a build/web folder should be created in the current directory, containing the files needed to run the game in a web browser (index.html, the-labyrinth-of-gaze.apk, etc.).\nHere\u0026rsquo;s an example of the built files: /game/the-labyrinth-of-gaze/build/web/index.html\nEmbedding the Game in Your Hugo Blog Next, let\u0026rsquo;s integrate the built game into your Hugo blog. We\u0026rsquo;ll use the hugo-theme-stack theme as an example, but the basic principles should apply to other Hugo themes as well.\n1. Regarding the Location for Game Files Hugo has a convenient folder called static. Files and folders placed here are copied directly to the root of your built site when the site is generated. We\u0026rsquo;ll place the game files created by pygbag here.\nPlacement Steps (Example: Game name the-labyrinth-of-gaze):\nInside the static folder at the root of your Hugo project, create a folder named game (create it if it doesn\u0026rsquo;t exist). Inside static/game/, create another folder for your game (e.g., the-labyrinth-of-gaze). Copy the contents of the build/web folder generated by pygbag earlier into the static/game/the-labyrinth-of-gaze/ folder you just created. Note: This is an important point! Do not copy the build/web folder itself, but rather copy the files within it (index.html, the-labyrinth-of-gaze.apk, etc.). After placement, the folder structure should look something like this:\n1 2 3 4 5 6 7 8 9 (Your Hugo Project Folder)/ ‚îî‚îÄ‚îÄ static/ ‚îî‚îÄ‚îÄ game/ ‚îî‚îÄ‚îÄ the-labyrinth-of-gaze/ \u0026lt;-- Copy build results into this folder ‚îî‚îÄ‚îÄ build/ ‚îî‚îÄ‚îÄ web/ ‚îú‚îÄ‚îÄ index.html ‚îú‚îÄ‚îÄ the-labyrinth-of-gaze.apk ‚îî‚îÄ‚îÄ (Other necessary files) ... Point: This setup allows you to access the game\u0026rsquo;s index.html later from your website with a URL like /game/the-labyrinth-of-gaze/build/web/index.html.\n2. Writing \u0026lt;iframe\u0026gt; Tags Every Time is Tedious, So Let\u0026rsquo;s Create a Hugo Shortcode Hugo has a useful feature called shortcodes, so let\u0026rsquo;s use it to easily embed our game. Creating this will save you a lot of effort later.\nCreate a new file named game-iframe.html inside the layouts/shortcodes/ folder of your Hugo project and paste the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 {{/* layouts/shortcodes/game-iframe.html */}} {{/* Receive the game URL via \u0026#39;src\u0026#39; */}} {{ $src := .Get \u0026#34;src\u0026#34; }} {{/* Receive the aspect ratio via \u0026#39;aspect-ratio\u0026#39; (defaults to 75% = 4:3 if not specified) */}} {{ $aspectRatio := .Get \u0026#34;aspect-ratio\u0026#34; | default \u0026#34;75%\u0026#34; }} {{/* Responsive iframe embedding styles */}} \u0026lt;div style=\u0026#34;position: relative; padding-bottom: {{ $aspectRatio }}; height: 0; overflow: hidden; max-width: 100%; height: auto;\u0026#34;\u0026gt; \u0026lt;iframe src=\u0026#34;{{ $src }}\u0026#34; style=\u0026#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: 1px solid #ccc;\u0026#34; title=\u0026#34;Embedded Game\u0026#34; sandbox=\u0026#34;allow-scripts allow-same-origin allow-pointer-lock allow-fullscreen\u0026#34; loading=\u0026#34;lazy\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; Here\u0026rsquo;s what this shortcode does:\nAccepts the URL of the game to embed via the src parameter. Allows specifying the visual aspect ratio of the game screen via the aspect-ratio parameter (e.g., 75% for 4:3, 56.25% for 16:9). It defaults to 75% (similar to 4:3) if not specified. It embeds the content from the specified URL using an \u0026lt;iframe\u0026gt;. Uses CSS to maintain the aspect ratio even when the screen size changes (making it responsive). Applies some security restrictions to the iframe content using the sandbox attribute. Uses loading=\u0026quot;lazy\u0026quot; as a small optimization to defer loading the iframe until it\u0026rsquo;s close to the viewport, potentially speeding up initial page load. 3. Alright, Setup is Complete! Let\u0026rsquo;s Use it in a Post. Now you\u0026rsquo;re ready! Open the Markdown file for the post where you want to feature the game and use the shortcode you just created.\nFor example, create a post file like content/posts/my-pygame-game.md and write something like this in the body:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 --- title: \u0026#34;Publishing My Pygame Game: The Labyrinth of Gaze!\u0026#34; # Post title date: 2025-03-28T00:00:00+09:00 description: \u0026#34;Published my maze game made with Pygame and pygbag. Play it easily in your browser!\u0026#34; # Post description slug: the-labyrinth-of-gaze-game # Post slug (part of the URL) image: the-labyrinth-of-gaze.webp # Featured image categories: [\u0026#34;Games\u0026#34;] # Category tags: [\u0026#34;Pygame\u0026#34;, \u0026#34;Indie Game\u0026#34;, \u0026#34;Puzzle\u0026#34;] # Tags draft: false --- I tried publishing my \u0026#34;Labyrinth of Gaze\u0026#34; game, made with Pygame, on the web! Using `pygbag`, you can embed it in a blog like this. Convenient! Feel free to play it right here in your browser. {{\u0026lt; game-iframe src=\u0026#34;/game/the-labyrinth-of-gaze/build/web/index.html\u0026#34; aspect-ratio=\u0026#34;75%\u0026#34; \u0026gt;}} Controls: * (Describe the game controls here) * Example: Arrow keys to move, Spacebar to jump, etc. Game Description: (Describe the game\u0026#39;s rules, objectives, highlights, etc., here) I hope you enjoy it! The key points here are:\nInside {{\u0026lt; ... \u0026gt;}}, you write the name of the shortcode you created earlier: game-iframe. For the src parameter, specify the absolute path on your website (starting with /) to the game\u0026rsquo;s index.html file that you placed in the static folder. Example: If you placed it at static/game/the-labyrinth-of-gaze/build/web/index.html, you write /game/the-labyrinth-of-gaze/build/web/index.html. Important: Be careful here, it\u0026rsquo;s easy to get wrong! Make sure this path correctly points to where you placed your game files. Adjust the aspect-ratio to match your game\u0026rsquo;s screen for better presentation (e.g., 56.25% for 16:9). Of course, you can add instructions, game descriptions, or any other content below the shortcode. After this, simply build your site with the hugo command and deploy it. The game should appear embedded in your post!\nSummary In this article, we\u0026rsquo;ve walked through the steps to use the convenient tool pygbag to convert a Pygame game to WebAssembly (WASM), place it in a Hugo blog\u0026rsquo;s static folder, and create a Hugo shortcode for easy iframe embedding within posts.\nHere are what I consider the main advantages of this method:\nNo special server setup required! Being able to publish the game using only static files is very convenient. Isn\u0026rsquo;t it exciting to be able to easily add interactive games directly into your blog posts? It increases the chances for more people to play the Pygame game you created! Visitors will appreciate being able to play instantly without needing any browser plugins. If you\u0026rsquo;ve made a game with Pygame and have been thinking, \u0026ldquo;I\u0026rsquo;d like to publish this on the web,\u0026rdquo; I encourage you to try these steps.\nThank you for reading!\nSource Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 # Maze exploration animation using Pygame with A* algorithm (Auto-progress \u0026amp; enhanced effects - modified version) import pygame import random import heapq import math import time # Added for dt calculation (clock.tick is also okay) # --- Constants --- # Grid settings GRID_WIDTH = 31 GRID_HEIGHT = 25 CELL_SIZE = 15 MARGIN = 1 # Window size WINDOW_WIDTH = GRID_WIDTH * (CELL_SIZE + MARGIN) + MARGIN WINDOW_HEIGHT = GRID_HEIGHT * (CELL_SIZE + MARGIN) + MARGIN # Colors (RGB) - Updated to a modern color scheme WHITE = (245, 245, 245) BLACK = (20, 20, 30) GREY = (180, 180, 180) GREEN = (76, 187, 23) RED = (235, 64, 52) BLUE = (66, 135, 245) YELLOW = (250, 204, 21) CYAN = (28, 186, 210) ORANGE = (255, 126, 28) LIGHT_ORANGE = (255, 183, 77) # For blinking PATH_HIGHLIGHT = (130, 210, 240) # Light blue (for path display animation) PATH_HIGHLIGHT_PULSE = (180, 230, 250) # For pulse effect GOAL_FLASH = (255, 255, 255) # For goal reached effect HOVER_COLOR = (220, 220, 220) # For hover effect PURPLE = (180, 120, 240) # New color PINK = (255, 105, 180) # New color # Animation speed (Frames Per Second) FPS = 60 # Wait time before auto-reset (seconds) RESET_DELAY_SECONDS = 2.0 # Path highlight animation speed (cells per frame, smaller is slower) PATH_HIGHLIGHT_SPEED = 0.3 # --- Helper functions (unchanged) --- def heuristic(a, b): (r1, c1) = a (r2, c2) = b return abs(r1 - r2) + abs(c1 - c2) def get_valid_neighbors(node, grid): neighbors = [] row, col = node rows = len(grid) cols = len(grid[0]) directions = [(0, 1), (0, -1), (1, 0), (-1, 0)] for dr, dc in directions: nr, nc = row + dr, col + dc if 0 \u0026lt;= nr \u0026lt; rows and 0 \u0026lt;= nc \u0026lt; cols and grid[nr][nc] == 0: neighbors.append((nr, nc)) return neighbors def reconstruct_path(came_from, current): path = [] while current in came_from: path.append(current) current = came_from[current] path.reverse() return path def generate_maze(width, height): grid = [[1 for _ in range(width)] for _ in range(height)] start_r, start_c = random.randrange(1, height, 2), random.randrange(1, width, 2) grid[start_r][start_c] = 0 stack = [(start_r, start_c)] visited = {(start_r, start_c)} while stack: cr, cc = stack[-1] neighbors = [] for dr, dc in [(0, 2), (0, -2), (2, 0), (-2, 0)]: nr, nc = cr + dr, cc + dc if 0 \u0026lt; nr \u0026lt; height - 1 and 0 \u0026lt; nc \u0026lt; width - 1 and (nr, nc) not in visited: neighbors.append((nr, nc)) if neighbors: nr, nc = random.choice(neighbors) grid[(cr + nr) // 2][(cc + nc) // 2] = 0 grid[nr][nc] = 0 visited.add((nr, nc)) stack.append((nr, nc)) else: stack.pop() passages = [(r, c) for r in range(height) for c in range(width) if grid[r][c] == 0] if len(passages) \u0026lt; 2: start_node = (1, 1) if height \u0026gt; 1 and width \u0026gt; 1 else (0, 0) end_node = (height - 2, width - 2) if height \u0026gt; 2 and width \u0026gt; 2 else start_node if grid[start_node[0]][start_node[1]] == 1: grid[start_node[0]][start_node[1]] = 0 if grid[end_node[0]][end_node[1]] == 1: grid[end_node[0]][end_node[1]] = 0 else: start_node = random.choice(passages) end_node = random.choice(passages) while end_node == start_node: end_node = random.choice(passages) return grid, start_node, end_node # Particle class definition - Improved for more diverse effects class Particle: def __init__(self, x, y, color, particle_type=\u0026#34;normal\u0026#34;): self.x = x self.y = y self.base_color = color # Keep the original color self.color = color self.particle_type = particle_type self.size = ( random.randint(2, 6) if particle_type == \u0026#34;normal\u0026#34; else random.randint(3, 8) ) self.speed = ( random.uniform(1, 5) * 50 # Speed adjustment (dt-based) if particle_type == \u0026#34;normal\u0026#34; else random.uniform(0.5, 3) * 50 # Speed adjustment (dt-based) ) self.angle = random.uniform(0, math.pi * 2) self.lifespan = ( random.uniform(0.5, 1.5) if particle_type == \u0026#34;normal\u0026#34; else random.uniform(1.0, 2.5) ) self.age = 0 self.pulse_rate = random.uniform(3.0, 6.0) # For pulse effect self.original_size = self.size # For size variation self.fade_in_duration = 0.3 # Fade-in duration self.fade_out_start_ratio = 0.7 # At what percentage of lifespan should fade-out start? # Number of vertices for star particles self.vertices = random.randint(4, 6) if particle_type == \u0026#34;star\u0026#34; else 0 # For trail particles self.trail = [] self.trail_length = 5 if particle_type == \u0026#34;trail\u0026#34; else 0 # For ripple effect if particle_type == \u0026#34;ripple\u0026#34;: self.size = 1 self.max_size = random.randint(15, 25) self.expand_speed = random.uniform(0.8, 1.2) * 30 # Speed adjustment (dt-based) self.lifespan = random.uniform(1.0, 1.5) self.speed = 0 # Ripple does not move def update(self, dt): self.x += math.cos(self.angle) * self.speed * dt self.y += math.sin(self.angle) * self.speed * dt self.age += dt # Update process according to particle type size_decay_rate = self.original_size / (self.lifespan * (1.0 - self.fade_out_start_ratio)) if self.lifespan \u0026gt; 0 else 1 if self.particle_type == \u0026#34;normal\u0026#34;: if self.age \u0026gt;= self.lifespan * self.fade_out_start_ratio: self.size = max(0, self.size - size_decay_rate * dt) elif self.particle_type == \u0026#34;pulse\u0026#34;: pulse = math.sin(self.age * self.pulse_rate) * 0.5 + 0.5 current_size_factor = 1.0 if self.age \u0026gt;= self.lifespan * self.fade_out_start_ratio: current_size_factor = max(0, 1 - (self.age - self.lifespan * self.fade_out_start_ratio) / (self.lifespan * (1.0 - self.fade_out_start_ratio))) self.size = self.original_size * (0.5 + pulse * 0.5) * current_size_factor elif self.particle_type == \u0026#34;fade_in\u0026#34;: if self.age \u0026lt; self.fade_in_duration: self.size = self.original_size * (self.age / self.fade_in_duration) elif self.age \u0026gt;= self.lifespan * self.fade_out_start_ratio: fade_out_duration = self.lifespan * (1.0 - self.fade_out_start_ratio) self.size = max(0, self.original_size * (1 - (self.age - self.lifespan * self.fade_out_start_ratio) / fade_out_duration)) else: self.size = self.original_size # Max size after fade-in and before fade-out elif self.particle_type == \u0026#34;trail\u0026#34;: self.trail.append((self.x, self.y)) if len(self.trail) \u0026gt; self.trail_length: self.trail.pop(0) if self.age \u0026gt;= self.lifespan * self.fade_out_start_ratio: self.size = max(0, self.size - size_decay_rate * dt * 0.5) # Trail disappears a bit slower elif self.particle_type == \u0026#34;ripple\u0026#34;: self.size = min(self.size + self.expand_speed * dt, self.max_size) elif self.particle_type == \u0026#34;star\u0026#34;: if self.age \u0026gt;= self.lifespan * self.fade_out_start_ratio: self.size = max(0, self.size - size_decay_rate * dt) else: # default or rainbow etc. if self.age \u0026gt;= self.lifespan * self.fade_out_start_ratio: self.size = max(0, self.size - size_decay_rate * dt) # Color change (hue changes over time - rainbow type) if self.particle_type == \u0026#34;rainbow\u0026#34;: hue_shift = (self.age * 100) % 360 # HSV -\u0026gt; RGB conversion (simplified version) r_val, g_val, b_val = 0, 0, 0 i = int(hue_shift / 60) % 6 f = hue_shift / 60 - i v = 1.0 # Value (brightness) s = 1.0 # Saturation p = v * (1 - s) q = v * (1 - f * s) t = v * (1 - (1 - f) * s) if i == 0: r_val, g_val, b_val = v, t, p elif i == 1: r_val, g_val, b_val = q, v, p elif i == 2: r_val, g_val, b_val = p, v, t elif i == 3: r_val, g_val, b_val = p, q, v elif i == 4: r_val, g_val, b_val = t, p, v elif i == 5: r_val, g_val, b_val = v, p, q self.color = (int(r_val*255), int(g_val*255), int(b_val*255)) def draw(self, surface): if self.size \u0026lt;= 0: # Do not draw if size is 0 or less return # Calculate transparency for fade-in/out effects alpha = 255 if self.particle_type == \u0026#34;ripple\u0026#34;: # Calculate transparency for ripple effect (gradually fades) progress = self.age / self.lifespan if self.lifespan \u0026gt; 0 else 1 alpha = max(0, min(255, int(255 * (1 - progress) * 0.8))) # Become more transparent towards the end elif self.particle_type == \u0026#34;fade_in\u0026#34;: if self.age \u0026lt; self.fade_in_duration: alpha = int(255 * (self.age / self.fade_in_duration)) elif self.age \u0026gt;= self.lifespan * self.fade_out_start_ratio: fade_out_duration = self.lifespan * (1.0 - self.fade_out_start_ratio) if fade_out_duration \u0026gt; 0: alpha = max(0, min(255, int(255 * (1 - (self.age - self.lifespan * self.fade_out_start_ratio) / fade_out_duration)))) else: alpha = 0 # Just in case else: alpha = 255 else: # Normal, Pulse, Star, Trail, Rainbow etc. # Common fade-out process if self.age \u0026gt;= self.lifespan * self.fade_out_start_ratio: fade_out_duration = self.lifespan * (1.0 - self.fade_out_start_ratio) if fade_out_duration \u0026gt; 0: alpha = max(0, min(255, int(255 * (1 - (self.age - self.lifespan * self.fade_out_start_ratio) / fade_out_duration)))) else: alpha = 0 else: alpha = 255 # Validate and set color try: current_color = self.color if self.particle_type == \u0026#34;rainbow\u0026#34; else self.base_color if isinstance(current_color, tuple) and len(current_color) == 3: r = max(0, min(255, int(current_color[0]))) g = max(0, min(255, int(current_color[1]))) b = max(0, min(255, int(current_color[2]))) final_color = (r, g, b, alpha) else: final_color = (255, 255, 255, alpha) # Default color # Drawing according to particle type if self.particle_type == \u0026#34;ripple\u0026#34;: # Ripple effect (draw outline) line_width = max(1, int(self.max_size / 15 * (1 - self.age / self.lifespan))) # Outline gradually becomes thinner if self.size \u0026gt;= 1: # Minimum radius 1 or more pygame.draw.circle(surface, final_color, (int(self.x), int(self.y)), int(self.size), width=line_width) elif self.particle_type == \u0026#34;star\u0026#34; and self.vertices \u0026gt; 0: # Star particle points = [] outer_radius = self.size inner_radius = self.size * 0.4 for i in range(self.vertices * 2): angle = math.pi / self.vertices * i - math.pi / 2 # Adjust so the vertex is at the top radius = outer_radius if i % 2 == 0 else inner_radius x_p = self.x + math.cos(angle) * radius y_p = self.y + math.sin(angle) * radius points.append((x_p, y_p)) if len(points) \u0026gt;= 3: # At least 3 points required pygame.draw.polygon(surface, final_color, points) elif self.particle_type == \u0026#34;trail\u0026#34; and len(self.trail) \u0026gt; 1: # Trail particle for i in range(len(self.trail) - 1): start_pos = self.trail[i] end_pos = self.trail[i + 1] # Adjust trail alpha and width trail_alpha = alpha * ((i + 1) / len(self.trail))**2 # Fainter towards the end trail_width = max(1, int(self.size * ((i + 1) / len(self.trail)))) trail_color_tuple = (final_color[0], final_color[1], final_color[2], int(trail_alpha)) pygame.draw.line(surface, trail_color_tuple, start_pos, end_pos, trail_width) # Also draw the circle at the tip pygame.draw.circle(surface, final_color, (int(self.x), int(self.y)), int(self.size)) else: # Normal circular particles (Normal, Pulse, Fade_in, Rainbow) pygame.draw.circle(surface, final_color, (int(self.x), int(self.y)), int(self.size)) except (ValueError, TypeError) as e: # Use default color if an error occurs print(f\u0026#34;Error drawing particle: {e}, color={self.color}, alpha={alpha}, size={self.size}\u0026#34;) try: safe_color = (255, 255, 255, alpha) if self.size \u0026gt;= 1: pygame.draw.circle(surface, safe_color, (int(self.x), int(self.y)), int(max(1, self.size))) # Ensure minimum size of 1 except Exception as final_e: print(f\u0026#34;Final fallback drawing failed: {final_e}\u0026#34;) # --- Pygame Initialization --- pygame.init() screen = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT)) pygame.display.set_caption(\u0026#34;A* Maze Solver Animation (Auto-Repeat, ESC: Quit)\u0026#34;) clock = pygame.time.Clock() font = pygame.font.Font(None, 24) # --- State Variables --- grid = [] start_node = None end_node = None open_set_heap = [] open_set_map = {} closed_set = set() came_from = {} g_score = {} path = [] current_node = None solving = False maze_generated = False message = \u0026#34;\u0026#34; particles = [] # List for particles (ripples also integrated here) # ripples = [] # Removed as it\u0026#39;s no longer needed node_pulses = [] # For pulse effect during node search (currently might be unused?) # --- Auto-Reset Variables --- reset_timer = 0 # Wait frame counter RESET_DELAY_FRAMES = int(RESET_DELAY_SECONDS * FPS) # Convert seconds to number of frames start_reset_timer_after_highlight = False # Flag to start timer after highlighting is complete # --- Path Highlight Animation Variables --- path_highlight_index = 0.0 # Use float to advance slowly highlighting_path = False # --- Goal Reached Effect --- goal_reached_flash = False # Is it the frame immediately after reaching the goal? # --- Main Loop --- running = True frame_count = 0 # For blinking animation hover_cell = None # Currently hovered cell while running: # --- Delta Time Calculation --- dt = clock.tick(FPS) / 1000.0 # Delta time in seconds (avoid division by zero) if dt == 0: dt = 1 / FPS # Ensure minimum time step # --- Event Handling --- for event in pygame.event.get(): if event.type == pygame.QUIT: running = False if event.type == pygame.KEYDOWN: if event.key == pygame.K_ESCAPE: running = False # Get hovered cell from mouse coordinates mouse_pos = pygame.mouse.get_pos() mouse_col = mouse_pos[0] // (CELL_SIZE + MARGIN) mouse_row = mouse_pos[1] // (CELL_SIZE + MARGIN) if 0 \u0026lt;= mouse_row \u0026lt; GRID_HEIGHT and 0 \u0026lt;= mouse_col \u0026lt; GRID_WIDTH: hover_cell = (mouse_row, mouse_col) else: hover_cell = None # --- State Update --- if not maze_generated: # Reset wait timer reset_timer = 0 start_reset_timer_after_highlight = False highlighting_path = False path_highlight_index = 0.0 goal_reached_flash = False hover_cell = None particles = [] # Clear existing particles message = \u0026#34;Generating new maze...\u0026#34; screen.fill(BLACK) msg_render = font.render(message, True, WHITE) screen.blit(msg_render, (10, WINDOW_HEIGHT // 2 - 10)) pygame.display.flip() grid, start_node, end_node = generate_maze(GRID_WIDTH, GRID_HEIGHT) open_set_heap = [] open_set_map = {} closed_set = set() came_from = {} path = [] current_node = None g_score = { (r, c): float(\u0026#34;inf\u0026#34;) for r in range(GRID_HEIGHT) for c in range(GRID_WIDTH) } if start_node: # Confirm start_node is not None g_score[start_node] = 0 h_start = heuristic(start_node, end_node) if end_node else 0 f_start = g_score[start_node] + h_start heapq.heappush(open_set_heap, (f_start, h_start, start_node)) open_set_map[start_node] = (f_start, h_start) maze_generated = True solving = True if start_node and end_node else False # Do not solve if start/end nodes are missing message = \u0026#34;Solving...\u0026#34; if solving else \u0026#34;Maze generated (No start/end?)\u0026#34; # --- A* Algorithm Step Execution --- if solving and open_set_heap: current_f, current_h, current_node_popped = heapq.heappop(open_set_heap) # Skip if removed from open_set_map or a better path was found later if current_node_popped not in open_set_map or open_set_map[current_node_popped] \u0026gt; (current_f, current_h): pass # Ignore and proceed to the next loop else: # Remove from open_set_map because it\u0026#39;s being processed (may be re-added) # Since it\u0026#39;s targeted for processing when popped from heapq, del might be unnecessary. Duplication check is done in the if statement above. # del open_set_map[current_node_popped] # Deletion here might be unnecessary current_node = current_node_popped if current_node == end_node: path = reconstruct_path(came_from, current_node) solving = False message = \u0026#34;Goal Reached! Highlighting path...\u0026#34; current_node = None highlighting_path = True path_highlight_index = 0.0 goal_reached_flash = True # Effect generation flag ON start_reset_timer_after_highlight = True else: closed_set.add(current_node) # Definitely remove from open_set_map (because it entered closed_set) if current_node in open_set_map: del open_set_map[current_node] # Add ripple effect to the node being explored node_x = (current_node[1] * (CELL_SIZE + MARGIN)) + MARGIN + CELL_SIZE // 2 node_y = (current_node[0] * (CELL_SIZE + MARGIN)) + MARGIN + CELL_SIZE // 2 # Generate ripple effect (using Particle class) particles.append(Particle(node_x, node_y, YELLOW, \u0026#34;ripple\u0026#34;)) # Change color to YELLOW # Generate a small amount of small particles (during exploration) if random.random() \u0026lt; 0.1: # Slightly lower the probability for _ in range(1): # Reduce the number color = random.choice([YELLOW, ORANGE]) # Match color to exploration color particles.append(Particle(node_x, node_y, color, \u0026#34;fade_in\u0026#34;)) for neighbor in get_valid_neighbors(current_node, grid): if neighbor in closed_set: continue tentative_g_score = g_score[current_node] + 1 # Ignore if this path is not better than the existing one, or if a better path already exists in the open set # Note: open_set_map stores (f, h) neighbor_in_open = open_set_map.get(neighbor) if neighbor_in_open and tentative_g_score \u0026gt;= g_score.get(neighbor, float(\u0026#39;inf\u0026#39;)): continue # If a better path is found, or visiting for the first time came_from[neighbor] = current_node g_score[neighbor] = tentative_g_score h_neighbor = heuristic(neighbor, end_node) f_neighbor = tentative_g_score + h_neighbor # Add if not in open_set, update if present (heapq doesn\u0026#39;t support direct update, so add a new element) heapq.heappush(open_set_heap, (f_neighbor, h_neighbor, neighbor)) open_set_map[neighbor] = (f_neighbor, h_neighbor) # Save f, h elif solving and not open_set_heap: solving = False message = f\u0026#34;No path found! Resetting in {RESET_DELAY_SECONDS:.1f}s...\u0026#34; current_node = None reset_timer = RESET_DELAY_FRAMES # Start timer immediately on exploration failure # --- Path Highlight Processing --- if highlighting_path and path: if path_highlight_index \u0026lt; len(path): path_highlight_index += PATH_HIGHLIGHT_SPEED * FPS * dt # Adjust speed using dt # Processing at the moment of completion if path_highlight_index \u0026gt;= len(path): path_highlight_index = len(path) if start_reset_timer_after_highlight: reset_timer = RESET_DELAY_FRAMES message = f\u0026#34;Path complete! Resetting in {RESET_DELAY_SECONDS:.1f}s...\u0026#34; start_reset_timer_after_highlight = False # --- Auto-Reset Timer Processing --- if reset_timer \u0026gt; 0: reset_timer -= 1 # Countdown on a frame basis remaining_time = reset_timer / FPS # Convert to seconds for display if not solving and not path: message = f\u0026#34;No path found! Resetting in {remaining_time:.1f}s...\u0026#34; elif not solving and path and path_highlight_index \u0026gt;= len(path): message = f\u0026#34;Path complete! Resetting in {remaining_time:.1f}s...\u0026#34; if reset_timer \u0026lt;= 0: maze_generated = False # --- Drawing Process --- # Gradient background for y in range(WINDOW_HEIGHT): time_factor = math.sin(frame_count * 0.005) * 0.2 r_base = 30 + int(10 * time_factor) g_base = 40 + int(15 * time_factor) b_base = 60 + int(20 * time_factor) gradient_factor = math.sin(math.pi * y / WINDOW_HEIGHT) r = int(r_base + (50 - r_base) * gradient_factor) # Adjust to be slightly darker g = int(g_base + (70 - g_base) * gradient_factor) # Adjust to be slightly darker b = int(b_base + (90 - b_base) * gradient_factor) # Adjust to be slightly darker pygame.draw.line(screen, (max(0,r), max(0,g), max(0,b)), (0, y), (WINDOW_WIDTH, y)) # Improve cell texture (shadow and gloss) - This part can remain as is shadow_surface = pygame.Surface((WINDOW_WIDTH, WINDOW_HEIGHT), pygame.SRCALPHA) for row in range(GRID_HEIGHT): for col in range(GRID_WIDTH): rect = pygame.Rect( (MARGIN + CELL_SIZE) * col + MARGIN, (MARGIN + CELL_SIZE) * row + MARGIN, CELL_SIZE, CELL_SIZE, ) if grid[row][col] == 0: # Passage pygame.draw.rect(shadow_surface, (0, 0, 0, 30), rect.inflate(1, 1), border_radius=3) # Slightly lighter shadow light_rect = rect.inflate(-3, -3).move(-1, -1) pygame.draw.rect(shadow_surface, (255, 255, 255, 50), light_rect, border_radius=2) # Gloss is also slightly subdued else: # Wall pygame.draw.rect(shadow_surface, (0, 0, 0, 20), rect.inflate(1, 1), border_radius=2) pygame.draw.rect(shadow_surface, (0, 0, 0, 30), rect.inflate(-2, -2), border_radius=1, width=1) # Inner shadow screen.blit(shadow_surface, (0, 0)) # Particle generation upon reaching the goal if goal_reached_flash: goal_x = (end_node[1] * (CELL_SIZE + MARGIN)) + MARGIN + CELL_SIZE // 2 goal_y = (end_node[0] * (CELL_SIZE + MARGIN)) + MARGIN + CELL_SIZE // 2 # Generate diverse particle types for _ in range(40): # Increase normal particles color = random.choice([RED, YELLOW, ORANGE, BLUE, GREEN, PURPLE, PINK, WHITE]) particles.append(Particle(goal_x, goal_y, color, \u0026#34;normal\u0026#34;)) for _ in range(15): # Increase star particles color = random.choice([YELLOW, WHITE, ORANGE, CYAN]) particles.append(Particle(goal_x, goal_y, color, \u0026#34;star\u0026#34;)) for _ in range(10): # Pulse effect color = random.choice([CYAN, PURPLE, PINK, BLUE]) particles.append(Particle(goal_x, goal_y, color, \u0026#34;pulse\u0026#34;)) for _ in range(8): # Trail particle color = random.choice([BLUE, CYAN, WHITE, GREEN]) particles.append(Particle(goal_x, goal_y, color, \u0026#34;trail\u0026#34;)) for _ in range(10): # Rainbow particle particles.append(Particle(goal_x, goal_y, WHITE, \u0026#34;rainbow\u0026#34;)) # Initial color white is fine for _ in range(6): # Also generate ripple effect as Particle color = random.choice([WHITE, CYAN, BLUE, YELLOW]) # Ripple color particles.append(Particle(goal_x, goal_y, color, \u0026#34;ripple\u0026#34;)) # Generate with ripple type goal_reached_flash = False # ‚òÖ‚òÖ‚òÖ Reset the flag immediately after particle generation ‚òÖ‚òÖ‚òÖ # Cell drawing loop for row in range(GRID_HEIGHT): for col in range(GRID_WIDTH): color = WHITE if grid[row][col] == 1: color = BLACK node = (row, col) is_path_node = False # Flag indicating whether it is a target for path highlighting # --- Color setting according to cell state --- if node in closed_set: # Color for closed list (explored) - Slightly darker CYAN color = (20, 140, 160) # Node in open_set_map (exploration candidate) - Slightly darker YELLOW # Even if the same node exists multiple times in heapq, open_set_map should contain the latest (f,h) if node in open_set_map: color = (200, 160, 10) # Slightly darker YELLOW # --- Path Highlighting --- if highlighting_path and path: current_path_segment_index = int(path_highlight_index) if node in path[:current_path_segment_index]: is_path_node = True pulse_factor = math.sin(frame_count * 0.15 + path.index(node) * 0.1) * 0.5 + 0.5 # Phase shift based on node position r = int(PATH_HIGHLIGHT[0] + (PATH_HIGHLIGHT_PULSE[0] - PATH_HIGHLIGHT[0]) * pulse_factor) g = int(PATH_HIGHLIGHT[1] + (PATH_HIGHLIGHT_PULSE[1] - PATH_HIGHLIGHT[1]) * pulse_factor) b = int(PATH_HIGHLIGHT[2] + (PATH_HIGHLIGHT_PULSE[2] - PATH_HIGHLIGHT[2]) * pulse_factor) color = (r, g, b) # Effect for the leading node if current_path_segment_index \u0026lt; len(path) and node == path[current_path_segment_index - 1]: if (frame_count // 4) % 2 == 0: # Adjust blink speed color = PATH_HIGHLIGHT_PULSE # Particle at the tip (low probability) if random.random() \u0026lt; 0.15: # Slightly increase probability x = (node[1] * (CELL_SIZE + MARGIN)) + MARGIN + CELL_SIZE // 2 y = (node[0] * (CELL_SIZE + MARGIN)) + MARGIN + CELL_SIZE // 2 particles.append(Particle(x, y, PATH_HIGHLIGHT_PULSE, \u0026#34;fade_in\u0026#34;)) # Match the color # --- Currently Explored Node --- if solving and node == current_node: # Blinking effect if (frame_count // 8) % 2 == 0: # Adjust blink speed color = LIGHT_ORANGE else: color = ORANGE # --- Start and Goal --- if node == start_node: color = GREEN elif node == end_node: # Flash immediately after reaching the goal is not managed by the goal_reached_flash flag, # other methods like making it brighter for the first few frames when highlighting_path becomes True can be considered # Currently kept simple as RED color = RED # --- Cell Drawing --- rect = pygame.Rect( (MARGIN + CELL_SIZE) * col + MARGIN, (MARGIN + CELL_SIZE) * row + MARGIN, CELL_SIZE, CELL_SIZE, ) pygame.draw.rect(screen, color, rect, border_radius=3) # --- Gloss and Hover Effect --- is_floor_like = (grid[row][col] == 0 or node == start_node or node == end_node or node in open_set_map or node in closed_set or is_path_node) if is_floor_like: # Gloss highlight_rect = rect.copy() highlight_rect.height = max(1, CELL_SIZE // 4) # Slightly smaller highlight_color = (min(255, color[0] + 40), min(255, color[1] + 40), min(255, color[2] + 40)) pygame.draw.rect(screen, highlight_color, highlight_rect, border_top_left_radius=3, border_top_right_radius=3) # Hover if hover_cell == node: hover_rect = rect.inflate(-1, -1) # To avoid overlapping with the border hover_color = HOVER_COLOR # Fixed color might be clearer # pygame.draw.rect(screen, hover_color, hover_rect, border_radius=2) # Fill pygame.draw.rect(screen, hover_color, hover_rect, width=1, border_radius=2) # Display with border # --- Border Line --- border_color = (max(0, color[0] - 50), max(0, color[1] - 50), max(0, color[2] - 50)) # Darker pygame.draw.rect(screen, border_color, rect, 1, border_radius=3) frame_count += 1 # Increment frame_count here # --- Particle Update and Drawing --- active_particles = [] for p in particles: p.update(dt) # Pass dt for update # Survival check based on lifespan and size (or reaching max size for ripples) is_alive = p.age \u0026lt; p.lifespan if p.particle_type == \u0026#34;ripple\u0026#34;: # Ripple disappears when lifespan ends (keeps moving even after reaching max_size) pass else: # Normal particles disappear when size becomes 0 is_alive = is_alive and p.size \u0026gt; 0 if is_alive: active_particles.append(p) particles = active_particles # Keep only active particles # Create a transparent Surface for particle drawing # Using SRCALPHA ensures that the alpha value (transparency) of each particle is handled correctly particle_surface = pygame.Surface((WINDOW_WIDTH, WINDOW_HEIGHT), pygame.SRCALPHA) for p in particles: p.draw(particle_surface) # Draw on the transparent Surface # Blit particle_surface onto the screen (where background and cells are already drawn) screen.blit(particle_surface, (0, 0)) # --- Message Display --- if message: text_color = WHITE stroke_color = BLACK msg_render = font.render(message, True, text_color) # Draw stroke for dx, dy in [(-1,-1), (-1,1), (1,-1), (1,1), (-1,0), (1,0), (0,-1), (0,1)]: stroke_render = font.render(message, True, stroke_color) screen.blit(stroke_render, (10 + dx, WINDOW_HEIGHT - 25 + dy)) # Draw main text screen.blit(msg_render, (10, WINDOW_HEIGHT - 25)) # --- Screen Update --- pygame.display.flip() # Resetting goal_reached_flash moved to immediately after particle generation # --- Cleanup Process --- pygame.quit() ","date":"2025-03-30T05:18:12+09:00","image":"https://takoyakisoft.com/p/publish-pygame-web-hugo-pygbag/publish-pygame-web-hugo-pygbag_hu_5faf9495e623c6de.webp","permalink":"https://takoyakisoft.com/en/p/publish-pygame-web-hugo-pygbag/","title":"Publish Your Pygame Game on the Web! Easily Embed in Hugo Blog with pygbag (WebAssembly)"},{"content":"How to Pay $5 to xAI and Get $150 in Credits Every Month I registered for xAI by following the steps outlined in this article:\nGrok API: Pay $5 and Get $150 Worth of Usage?\nStart by visiting the xAI Cloud Console.\nClick in the order of 1 and 2 as shown in the image to enter your \u0026ldquo;Billing address.\u0026rdquo;\nThe image is just an example; I filled it out in English. It should match the address associated with your credit card.\nI left the \u0026ldquo;Tax ID Type\u0026rdquo; and \u0026ldquo;Tax Number\u0026rdquo; fields blank since they‚Äôre optional. I‚Äôm not entirely sure about the tax implications here.\nNext, enter your \u0026ldquo;Payment methods.\u0026rdquo;\nI used a PayPay JCB card, and it worked fine. (Note: PayPay is a payment service popular in Japan, but any valid credit card should work.)\nThere‚Äôs a field labeled \u0026ldquo;Redeem Promo code,\u0026rdquo; but as of February 20, I couldn‚Äôt find any promo codes even after searching with xAI‚Äôs DeepResearch tool. It seems there aren‚Äôt any available right now.\nTo buy credits, click \u0026ldquo;Purchase credits.\u0026rdquo;\nA $5 purchase is sufficient. After paying $5, a \u0026ldquo;Share Data\u0026rdquo; button will appear (I forgot to capture a screenshot of this).\nAs explained in this article, by agreeing to share data with xAI, you can receive $150 in credits every month:\nGet $150 in free API credits each month\nHow to Use xAI with VSCode Extensions Using Grok API with Cody Cody: AI Code Assistant is an AI-powered extension that offers code completion, editing, and chat features.\nCode completion is always free, but code editing and chat come with usage limits. By integrating the Grok API for editing and chat, you can unlock an unrestricted AI coding experience for just $5.\nAs shown in the image, navigate to \u0026ldquo;File\u0026rdquo; ‚Üí \u0026ldquo;Preferences\u0026rdquo; ‚Üí \u0026ldquo;Settings,\u0026rdquo; then click the icon in the top-right corner to open \u0026ldquo;settings.json.\u0026rdquo;\nAlternatively, press \u0026ldquo;Ctrl\u0026rdquo; + \u0026ldquo;Shift\u0026rdquo; + P and select \u0026ldquo;Preferences: Open Settings (JSON).\u0026rdquo;\nHere\u0026rsquo;s how to set up the models you want to add inside \u0026quot;cody.dev.models\u0026quot;. Since Grok is an OpenAI-compatible API, specify \u0026quot;openai\u0026quot; as the provider.\nAdditionally, I\u0026rsquo;ve set up the DeepSeek and Gemini APIs as well. DeepSeek is also an affordable API that you can use.\nGemini is an API provided by Google and is free to use. (Since there are restrictions on continuous usage, I\u0026rsquo;m using multiple instances.)\nFor \u0026quot;inputTokens\u0026quot;, \u0026quot;temperature\u0026quot;, and \u0026quot;stream\u0026quot;, I\u0026rsquo;m using the official examples as they are.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 { \u0026#34;cody.dev.models\u0026#34;: [ { \u0026#34;provider\u0026#34;: \u0026#34;openai\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;grok-2-latest\u0026#34;, \u0026#34;inputTokens\u0026#34;: 131072, \u0026#34;outputTokens\u0026#34;: 8192, \u0026#34;apiKey\u0026#34;: \u0026#34;xai-xxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;temperature\u0026#34;: 0, \u0026#34;stream\u0026#34;: false }, \u0026#34;apiEndpoint\u0026#34;: \u0026#34;https://api.x.ai/v1\u0026#34; }, { \u0026#34;provider\u0026#34;: \u0026#34;groq\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;deepseek-chat\u0026#34;, \u0026#34;inputTokens\u0026#34;: 128000, \u0026#34;outputTokens\u0026#34;: 8192, \u0026#34;apiKey\u0026#34;: \u0026#34;sk-xxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;temperature\u0026#34;: 0.6 }, \u0026#34;apiEndpoint\u0026#34;: \u0026#34;https://api.deepseek.com/chat/completions\u0026#34; }, { \u0026#34;provider\u0026#34;: \u0026#34;google\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;gemini-2.0-pro-exp-02-05\u0026#34;, \u0026#34;inputTokens\u0026#34;: 2097152, \u0026#34;outputTokens\u0026#34;: 8192, \u0026#34;apiKey\u0026#34;: \u0026#34;AIxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;temperature\u0026#34;: 1.0 } }, { \u0026#34;provider\u0026#34;: \u0026#34;google\u0026#34;, \u0026#34;model\u0026#34;: \u0026#34;gemini-2.0-flash-exp\u0026#34;, \u0026#34;inputTokens\u0026#34;: 1048576, \u0026#34;outputTokens\u0026#34;: 8192, \u0026#34;apiKey\u0026#34;: \u0026#34;AIxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;temperature\u0026#34;: 1.0 } } ], \u0026#34;cody.edit.preInstruction\u0026#34;: \u0026#34;Please think thoughts in English. Output should be in English.\u0026#34;, \u0026#34;cody.chat.preInstruction\u0026#34;: \u0026#34;Please think thoughts in English. Output should be in English.\u0026#34; } For code editing (\u0026quot;cody.edit.preInstruction\u0026quot;) and chat (\u0026quot;cody.chat.preInstruction\u0026quot;), you can set custom prompts, similar to those used in ChatGPT.\nUsing Grok API with Cline Cline seems to be an AI agent. It can read and write files, create new ones, and execute commands. Since it interacts with the API multiple times, it tends to consume a lot of tokens.\nCline can be configured using the \u0026ldquo;OpenAI Compatible\u0026rdquo; option, as shown in the image. Once you‚Äôve completed the setup, click \u0026ldquo;Done\u0026rdquo; to save your settings.\nUsing Grok API with Roo Code Roo Code (prev. Roo Cline) is a fork of Cline, offering faster updates and more features.\nRoo Code can also be set up using the \u0026ldquo;OpenAI Compatible\u0026rdquo; option, as shown in the image. After finishing the configuration, click \u0026ldquo;Done\u0026rdquo; to save.\n","date":"2025-02-20T00:00:00+09:00","image":"https://takoyakisoft.com/p/registering-xai-grok-api-and-using-it-with-vscode-extensions-cody-cline-roo-code/xai_payment_hu_caf0b26686df4180.webp","permalink":"https://takoyakisoft.com/en/p/registering-xai-grok-api-and-using-it-with-vscode-extensions-cody-cline-roo-code/","title":"Registering for xAI (Grok API) and Using It with VSCode Extensions (Cody, Cline, Roo Code)"},{"content":"YouTube Video Translate Entire eBook PDFs with Google Translate (Free!) Google Translate Access Google Translate at the following URL: https://translate.google.com/\nImages can be translated.\nThe original text is masked with a background color, and the translated text is superimposed. You can download the translated image using the \u0026ldquo;Download translation\u0026rdquo; button.\nFiles exceeding 10MB or 300 pages cannot be translated. The following file formats are supported:\n.docx .pdf .pptx .xlsx PDFsam Basic Download PDFsam Basic from the following URL: https://pdfsam.org/download-pdfsam-basic/ I downloaded the \u0026ldquo;Portable archive\u0026rdquo; version.\nIt appears to configure Java settings upon launch. Run it using the \u0026ldquo;pdfsam.bat\u0026rdquo; file.\nUse this option if the file size exceeds 10MB. It\u0026rsquo;s recommended to set \u0026ldquo;Split by file size\u0026rdquo; to 5.6MB. This is because splitting at 10MB sometimes caused timeout errors with Google Translate.\nUse this option if the page count exceeds 300 pages. Set \u0026ldquo;Split after every \u0026rsquo;n\u0026rsquo; pages\u0026rdquo; to 300. If you encounter timeouts, reduce this value to 200 or 100.\nDrag and drop the split PDF files as shown in the image.\nClick \u0026ldquo;Run\u0026rdquo; to merge the files. The merged file will have a default name, so renaming is necessary.\nPDFsam Visual This is not recommended as it only offers a 14-day trial. However, it\u0026rsquo;s mentioned here due to its effective compression of text-based PDFs.\nDownload PDFsam Visual from the following URL: https://pdfsam.org/download-pdfsam-visual/ I downloaded the \u0026ldquo;Portable archive 64-bit\u0026rdquo; version.\nSelect \u0026ldquo;Compress\u0026rdquo; on the start screen.\nDrag and drop your PDF file here.\nConfigure the settings: choose the output folder with \u0026ldquo;BROWSE\u0026rdquo; and click \u0026ldquo;SAVE\u0026rdquo; to compress.\niLovePDF Go to the iLovePDF \u0026ldquo;Compress PDF\u0026rdquo; page at the following URL: https://www.ilovepdf.com/compress_pdf\nDrag and drop your PDF file here.\nSelect \u0026ldquo;Extreme compression\u0026rdquo; and click \u0026ldquo;Compress PDF\u0026rdquo;.\n","date":"2024-11-24T17:00:00+09:00","image":"https://takoyakisoft.com/p/free-translate-entire-ebook-pdfs-with-google-translate/free-translate-entire-ebook-pdfs-with-google-translate_hu_6e991b2d48c69724.webp","permalink":"https://takoyakisoft.com/en/p/free-translate-entire-ebook-pdfs-with-google-translate/","title":"Translate Entire eBook PDFs with Google Translate (Free!)"},{"content":"YouTube Video Play Minecraft Remotely! Smooth Gameplay on Your Laptop Setup on your Laptop (Steam Link Client) Download Steam Link from the following URL: https://store.steampowered.com/remoteplay\nDownload the appropriate Steam Link version for your operating system from the location shown in this image.\nSteam Link is compatible with the following operating systems:\niPhone, iPad, Apple TV (11.0 or later) Android (5.0 or later) phones, tablets, and TVs Android users without Google Play access Raspberry Pi 3, 3+, 4 Windows Linux macOS Meta Quest 2, 3, Pro For Windows, it\u0026rsquo;s recommended to use the setup.exe file for installation instead of the msi. It will also check for any missing software required for your PC environment.\nClick \u0026ldquo;Next\u0026rdquo; through all the installation prompts.\nSetup on your Gaming PC (Steam Host) Go to \u0026ldquo;Steam\u0026rdquo; -\u0026gt; \u0026ldquo;Settings\u0026rdquo; -\u0026gt; \u0026ldquo;Remote Play\u0026rdquo; -\u0026gt; and toggle \u0026ldquo;Enable Remote Play\u0026rdquo; to ON.\nGo to \u0026ldquo;Games\u0026rdquo; -\u0026gt; \u0026ldquo;Add a Non-Steam Game to My Library\u0026rdquo; -\u0026gt; \u0026ldquo;Browse\u0026rdquo; and add the Minecraft Launcher. In my environment, it\u0026rsquo;s located on the D drive: \u0026ldquo;D:\\XboxGames\\Minecraft Launcher\\Content\\gamelaunchhelper.exe\u0026rdquo;. After opening, ensure \u0026ldquo;gamelaunchhelper\u0026rdquo; is checked and click \u0026ldquo;Add Selected Programs\u0026rdquo;.\nRemote Play from your Laptop (Steam Link Client) Make sure Steam is closed on your laptop. It will interfere with the connection.\nLaunch Steam Link. Allow access if prompted by the firewall. You may need to click an OK button on your gaming PC the first time you connect.\nThe mouse cursor might not appear. Connecting a spare mouse to the gaming PC can resolve this. Alternatively, if you are using Windows 11 Pro, logging in and out of a remote desktop session can also make the cursor appear.\nIt\u0026rsquo;s recommended to disable Raw Mouse Input in Minecraft\u0026rsquo;s settings. For some reason, moving the mouse cursor while opening and closing a chest could cause the viewpoint to shift in that direction. Disabling Raw Mouse Input should resolve this issue.\n","date":"2024-11-20T17:00:00+09:00","image":"https://takoyakisoft.com/p/steam-link-play-minecraft-remotely-smooth-gameplay-on-your-laptop/steam-link-play-minecraft-remotely-smooth-gameplay-on-your-laptop_hu_b0d1360ca7a52d1c.webp","permalink":"https://takoyakisoft.com/en/p/steam-link-play-minecraft-remotely-smooth-gameplay-on-your-laptop/","title":"Play Minecraft Remotely! Smooth Gameplay on Your Laptop"}]